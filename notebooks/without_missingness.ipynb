{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to perform a validation of methods used for multiple missing data imputation on a Cardiovascular Disease dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.getcwd() + '\\\\..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "RANDOM_STATE = 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cardio_train.csv', delimiter=';')\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['cardio'])\n",
    "y = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the results after each method\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Select columns to be scaled\n",
    "numeric_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
    "categorical_columns = ['gluc', 'smoke', 'alco', 'active', 'cholesterol', 'gender']\n",
    "\n",
    "# Fit and transform your data (only for numeric columns)\n",
    "scaler = StandardScaler()\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "# X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if the file exists\n",
    "file_path = \"../images/without_missingness/density_plots.png\"\n",
    "if not os.path.isfile(file_path):\n",
    "    # Set the size of the figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Loop through each column in X and generate a density plot\n",
    "    for i, feature_name in enumerate(df.columns):\n",
    "        # Set the subplot and plot the density of the column\n",
    "        plt.subplot(4, 4, i + 1)  # 4x4 grid, current subplot index\n",
    "        df[feature_name].plot(kind='density', color='blue', label=feature_name)\n",
    "        plt.title(feature_name)\n",
    "        plt.xlabel('Scaled Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    # Adjust the layout of the subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure as an image\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Check if the file exists\n",
    "file_path = \"../images/without_missingness/correlation_matrix.png\"\n",
    "if not os.path.isfile(file_path):\n",
    "    # Set the size of the figure\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Draw correlation matrix\n",
    "    sns.heatmap(X.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.title('Correlation Matrix')\n",
    "\n",
    "    # Save the figure as an image\n",
    "    plt.savefig('../images/without_missingness/correlation_matrix.png')\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing 10 subsets with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_of_data = 0.001\n",
    "\n",
    "def remove_features(num_features_to_remove=None, feature_indices_to_remove=None):\n",
    "    \"\"\"\n",
    "    Randomly removes features from a subset of data and replaces their values with NaN.\n",
    "    \n",
    "    Parameters:\n",
    "        num_features_to_remove (int): Number of features to remove randomly.\n",
    "        feature_indices_to_remove (array-like): Indices of features to remove.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Subset of data with removed features and NaN values.\n",
    "    \"\"\"\n",
    "    # Sample a subset of data\n",
    "    subset = X_train.sample(frac=fraction_of_data, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Determine features to remove based on number or indices provided\n",
    "    if feature_indices_to_remove is None:\n",
    "        if num_features_to_remove is None:\n",
    "            num_features_to_remove = np.random.randint(1, min(5, len(X_train.columns) - 1))\n",
    "        else:\n",
    "            features_to_remove = np.random.choice(subset.columns[:-1], num_features_to_remove, replace=False)\n",
    "    else:\n",
    "        features_to_remove = subset.columns[feature_indices_to_remove]\n",
    "    \n",
    "    # Replace values of selected features with NaN\n",
    "    for feature in features_to_remove:\n",
    "        subset[feature] = np.NaN\n",
    "    \n",
    "    return subset.astype('object')\n",
    "\n",
    "list_of_subsets = []\n",
    "subset_without_changes = X_train.sample(frac=fraction_of_data, random_state=RANDOM_STATE)\n",
    "\n",
    "# Generate subsets with varying numbers of removed features\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(1))\n",
    "\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(2))\n",
    "\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(3))\n",
    "\n",
    "for _ in range(1):\n",
    "    list_of_subsets.append(remove_features(4))\n",
    "\n",
    "# Print information about subsets and their missing columns\n",
    "print(f'Subsets with {list_of_subsets[0].shape[0]} datapoints and their columns with missing values:')\n",
    "for subset_index, current_row in enumerate(list_of_subsets):\n",
    "    nan_columns = current_row.columns[current_row.isnull().all()]\n",
    "    print(f\"Subset {subset_index+1}: {', '.join(nan_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_number_of_samples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot_encode_subset(subset):\n",
    "    # Set option to opt-in to future behavior\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    \n",
    "    # Apply one-hot encoding to categorical columns\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    subset_encoded = encoder.fit_transform(subset[categorical_columns])\n",
    "    column_names = encoder.get_feature_names_out(categorical_columns)\n",
    "    subset_encoded = pd.DataFrame(subset_encoded, columns=column_names)\n",
    "    \n",
    "    # Encode all possible categories from subset_without_changes\n",
    "    encoder.fit_transform(subset_without_changes[categorical_columns])\n",
    "    all_column_names = encoder.get_feature_names_out(categorical_columns)\n",
    "    \n",
    "    # Remove '.0' suffix from column names in subset_encoded\n",
    "    subset_encoded.columns = subset_encoded.columns.str.replace('.0', '')\n",
    "    \n",
    "    missing_columns = set(all_column_names) - set(subset_encoded.columns)\n",
    "    for col in missing_columns:\n",
    "        subset_encoded[col] = 0\n",
    "    \n",
    "    subset = subset.drop(categorical_columns, axis=1)\n",
    "    subset.loc[:, subset_encoded.columns] = subset_encoded\n",
    "    \n",
    "    # Reset option to default after processing\n",
    "    pd.set_option('future.no_silent_downcasting', False)\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from helpers.ConditionalGMM.condGMM import CondGMM\n",
    "import json\n",
    "\n",
    "def imputing_missing_data(subsets, method='simple', number_of_samples=default_number_of_samples, model=None):\n",
    "    \"\"\"\n",
    "    Impute missing data in subsets using different imputation methods.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Imputation method ('simple', 'multivariate', 'cgmm', 'vae', or 'gain').\n",
    "        model: Trained model for certain imputation methods.\n",
    "    \"\"\"\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        if method == 'simple':\n",
    "            # Simple Imputer\n",
    "            generated_data = simple_impute(subset)\n",
    "        else:\n",
    "            # Initialize to keep track of actual row index, because indices were shuffled\n",
    "            row_in_subset_index = 0\n",
    "            \n",
    "            for row_index, row in subset.iterrows():\n",
    "                # Get indices of unknown features\n",
    "                missing_features_indices = [row.index.get_loc(col) for col in row.index if pd.isna(row[col])]\n",
    "                \n",
    "                # If all features are known, continue\n",
    "                if len(missing_features_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                generated_data = None\n",
    "                \n",
    "                if method == 'multivariate' or method == 'cgmm':\n",
    "                    # Multivariate Imputer or Conditional GMM\n",
    "                    generated_data = cgmm_impute(model, missing_features_indices, row, number_of_samples)\n",
    "                elif method == 'vae':\n",
    "                    # Variational AutoEncoder\n",
    "                    generated_data = vae_impute(model, missing_features_indices, row, number_of_samples)\n",
    "                elif method == 'gain':\n",
    "                    # Generative Adversarial Imputation Network\n",
    "                    generated_data = gain_impute(model, missing_features_indices, row, number_of_samples)\n",
    "                    \n",
    "                # Update unknown features with sampled data\n",
    "                for feature_index in range(len(missing_features_indices)):\n",
    "                    subset.iloc[row_in_subset_index, missing_features_indices[feature_index]] = json.dumps([generated_data[sample_index][feature_index] for sample_index in range(generated_data.shape[0])])\n",
    "                \n",
    "                row_in_subset_index += 1\n",
    "\n",
    "def simple_impute(current_subset):\n",
    "    \"\"\"\n",
    "    Impute missing values using SimpleImputer with mean strategy.\n",
    "\n",
    "    Parameters:\n",
    "        current_subset (pandas.DataFrame): Subset of data with missing values.\n",
    "\n",
    "    This function iterates over each column in the given DataFrame and imputes missing values using\n",
    "    the SimpleImputer class from scikit-learn. The imputer is initialized with the 'mean' strategy,\n",
    "    which replaces missing values with the mean of the non-missing values in the column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a SimpleImputer object with 'mean' strategy\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in current_subset.columns:\n",
    "        # Check if any values in the column are missing\n",
    "        if pd.isna(current_subset[col]).any():\n",
    "            # Fit the imputer to the non-missing values in the column\n",
    "            imp.fit(subset_without_changes[[col]])\n",
    "            # Transform the missing values in the column\n",
    "            current_subset[col] = imp.transform(current_subset[[col]])\n",
    "            # if col in categorical_columns:\n",
    "            #     # Approximate categorical values to the nearest whole number\n",
    "            #     current_subset[col] = np.round(current_subset[col])\n",
    "\n",
    "def cgmm_impute(gmm, missing_features_indices, current_row, number_of_samples):\n",
    "    \"\"\"\n",
    "    Impute missing values using Conditional GMM.\n",
    "    \n",
    "    Parameters:\n",
    "        gmm (GMM): Gaussian Mixture Model.\n",
    "        missing_features_indices (list): Indices of missing features.\n",
    "        current_row (pandas.Series): Current row with missing values.\n",
    "        number_of_samples (int): Number of samples to generate.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Generated data with imputed missing values.\n",
    "    \"\"\"\n",
    "    # Find indices of known features\n",
    "    known_features_indices = [i for i in range(len(current_row))  # Iterate over indices\n",
    "                               if i not in missing_features_indices]  # Check if index is in missing_features_indices\n",
    "    \n",
    "    # Extract values of known features for the given row\n",
    "    known_features_values = current_row.iloc[known_features_indices]\n",
    "    \n",
    "    # Initialize CondGMM\n",
    "    cGMM = CondGMM(gmm.weights_, gmm.means_, gmm.covariances_, known_features_indices)\n",
    "    \n",
    "    # Generate samples using Conditional GMM\n",
    "    generated_data = cGMM.rvs(known_features_values, size=number_of_samples, random_state=RANDOM_STATE)\n",
    "    \n",
    "    return generated_data\n",
    "\n",
    "def vae_impute(model, missing_features_indices, current_row, number_of_samples):\n",
    "    \"\"\"\n",
    "    Impute missing values using a Variational Autoencoder.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Trained Variational Autoencoder model.\n",
    "        missing_features_indices (list): Indices of missing features.\n",
    "        current_row (pandas.Series): Current row with missing values.\n",
    "        number_of_samples (int): Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: Generated data with imputed missing values.\n",
    "    \"\"\"\n",
    "    generated_data = np.empty((number_of_samples, len(missing_features_indices)))\n",
    "    \n",
    "    # Repeat the prediction process for the specified number of samples\n",
    "    for _ in range(number_of_samples):\n",
    "        # Impute missing values for each feature index\n",
    "        # Reshape the missing values to a 2D array with one row and all missing features\n",
    "        missing_features_values = model.predict(current_row.values.reshape(1, -1).astype(np.float32), verbose=0)\n",
    "        \n",
    "        # Append the generated data to the list\n",
    "        generated_data = np.append(generated_data, missing_features_values[:, missing_features_indices], axis=0)\n",
    "    \n",
    "    return generated_data\n",
    "\n",
    "def gain_impute(model, missing_features_indices, current_row, number_of_samples):\n",
    "    \"\"\"\n",
    "    Impute missing values using a Generative Adversarial Imputation Network.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Trained GAIN model.\n",
    "        missing_features_indices (list): Indices of missing features.\n",
    "        current_row (pandas.Series): Current row with missing values.\n",
    "        number_of_samples (int): Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: Generated data with imputed missing values.\n",
    "    \"\"\"\n",
    "    generated_data = []\n",
    "    \n",
    "    # Repeat the prediction process for the specified number of samples\n",
    "    for _ in range(number_of_samples):\n",
    "        # Impute missing values for each feature index\n",
    "        # Reshape the missing values to a 2D array with one row and all missing features\n",
    "        missing_features_values = model.predict(current_row.values.reshape(1, -1).astype(np.float64), verbose=0)\n",
    "        \n",
    "        # Append the generated data to the list\n",
    "        generated_data.append(missing_features_values[:, missing_features_indices])\n",
    "        \n",
    "    # Convert the list of arrays to a numpy array\n",
    "    generated_data = np.concatenate(generated_data, axis=0)\n",
    "        \n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def get_scoring(subsets, method='simple', print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) scores for features in subsets of data.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Method used for imputation ('simple', 'multivariate', 'cgmm', or 'vae').\n",
    "        print_results (bool): Whether to print MSE scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing MSE scores for each feature in the subsets.\n",
    "    \"\"\"    \n",
    "    method = method.lower()\n",
    "    \n",
    "    # Deserialize any strings in subsets\n",
    "    for i in range(len(subsets)):\n",
    "        subsets[i] = subsets[i].map(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Initialize dictionary to store scores\n",
    "    all_subsets_scores = {}\n",
    "    \n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        feature_score = {}  # Initialize dictionary to store MSE values\n",
    "        \n",
    "        # Determine unknown features indexes dynamically for each subset\n",
    "        if method == 'simple':\n",
    "            missing_features_indices = [col_index for col_index, col in enumerate(list_of_subsets[subset_index].columns) if list_of_subsets[subset_index][col].isnull().all()]\n",
    "        else:\n",
    "            missing_features_indices = [col_index for col_index, col in enumerate(subset.columns) if subset[col].apply(lambda x: isinstance(x, list)).any()]\n",
    "\n",
    "        if not missing_features_indices:\n",
    "            continue  # Skip if there are no missing values\n",
    "        \n",
    "        # Iterate through rows in the subset DataFrame\n",
    "        for index, row in subset.iterrows():\n",
    "            original_values = X.iloc[index, missing_features_indices].values\n",
    "            \n",
    "            # Compute MSE for each feature separately\n",
    "            for feature_index in range(len(missing_features_indices)):\n",
    "                if method == 'simple':\n",
    "                    generated_samples = [row.iloc[missing_features_indices].values[feature_index]]\n",
    "                else:\n",
    "                    try:\n",
    "                        generated_samples_raw = row.iloc[missing_features_indices].values[feature_index]\n",
    "                        generated_samples = [sample for sample in generated_samples_raw if not pd.isna(sample)]\n",
    "                    except Exception as e:\n",
    "                        # TODO: Add handling for exceptions\n",
    "                        print(f\"Error processing row {index}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Grab the original value of the feature\n",
    "                original_value = original_values[feature_index]\n",
    "                \n",
    "                for sample in generated_samples:\n",
    "                    squared_error = (original_value - sample)**2\n",
    "                    \n",
    "                    if missing_features_indices[feature_index] not in feature_score:\n",
    "                        feature_score[missing_features_indices[feature_index]] = []\n",
    "                        \n",
    "                    feature_score[missing_features_indices[feature_index]].append(squared_error)\n",
    "        \n",
    "        for feature_index, score_values in feature_score.items():\n",
    "            mse = np.mean(score_values)\n",
    "            variance = np.var(subset_without_changes.iloc[:, feature_index])\n",
    "            if variance != 0:\n",
    "                nmse = mse / variance\n",
    "                feature_score[feature_index] = np.round(nmse, 3)\n",
    "            else:\n",
    "                feature_score[feature_index] = np.round(mse, 3)\n",
    "            \n",
    "        all_subsets_scores[subset_index] = feature_score\n",
    "        \n",
    "        # Print MSE scores if required\n",
    "        if print_results:\n",
    "            print(f\"MSE for Subset {subset_index + 1}:\")\n",
    "            for feature_index, score_values in feature_score.items():\n",
    "                print(f\"Feature {subset_without_changes.columns[feature_index]}: MSE = {score_values}\")\n",
    "    \n",
    "    # Return dictionary containing MSE scores\n",
    "    return all_subsets_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message=\"X does not have valid feature names\")\n",
    "\n",
    "# Load the trained classifier model\n",
    "classifier = load('..\\helpers\\predictive_models\\cardio_classifier.h5')\n",
    "\n",
    "def get_accuracy(subsets, method='simple', should_print=False):\n",
    "    \"\"\"\n",
    "    Calculate accuracy scores for subsets of data using a trained classifier.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Method used for imputation ('simple', 'multivariate', or 'cgmm').\n",
    "        should_print (bool): Whether to print accuracy scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of accuracy scores for each subset.\n",
    "    \"\"\"\n",
    "    method = method.lower()\n",
    "    \n",
    "    # Deserialize any strings in subsets\n",
    "    for i in range(len(subsets)):\n",
    "        subsets[i] = subsets[i].map(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    classification_results = []  # Initialize list to store classification results\n",
    "    accuracy_per_subset = []  # Initialize list to store accuracy scores\n",
    "\n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        subset_results = []  # Initialize list to store results for the current subset\n",
    "        \n",
    "        # Iterate through rows in the subset DataFrame\n",
    "        for row_index, row in subset.iterrows():\n",
    "            row_results = []  # Initialize list to store results for the current row\n",
    "            \n",
    "            # Process each row based on the method used\n",
    "            if method != 'simple':\n",
    "                serialized_arrays = []\n",
    "                non_serialized_values = []\n",
    "                \n",
    "                number_of_samples = 1\n",
    "                \n",
    "                # Split row values into serialized arrays and non-serialized values\n",
    "                for col, value in row.items():\n",
    "                    if isinstance(value, list):\n",
    "                        serialized_arrays.append((col, value))\n",
    "                        if number_of_samples < len(value):\n",
    "                            number_of_samples = len(value)\n",
    "                    else:\n",
    "                        non_serialized_values.append((col, value))\n",
    "                \n",
    "                # Generate combined rows by combining serialized arrays with non-serialized values\n",
    "                for i in range(number_of_samples):\n",
    "                    combined_row = non_serialized_values.copy()\n",
    "                    \n",
    "                    for col, serialized_array in serialized_arrays:\n",
    "                        if i < len(serialized_array):\n",
    "                            combined_row.append((col, serialized_array[i]))\n",
    "                    \n",
    "                    combined_row_array = [value for _, value in combined_row]\n",
    "                    \n",
    "                    try:\n",
    "                        result_array = classifier.predict([combined_row_array], verbose=0)\n",
    "                        row_results.append(result_array)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing row {row_index}: {e}\")\n",
    "            else:\n",
    "                # For simple method, predict directly from row values\n",
    "                result_array = classifier.predict([row.values.tolist()], verbose=0)\n",
    "                row_results.append(result_array)\n",
    "\n",
    "            subset_results.append(row_results)  # Append results for the current row to the subset results\n",
    "        \n",
    "        classification_results.append(subset_results)  # Append subset results to the classification results\n",
    "\n",
    "    # Calculate accuracy scores for each subset\n",
    "    for subset_index, subset_results in enumerate(classification_results):\n",
    "        true_labels = y.loc[subsets[subset_index].index]  # Get true labels for the current subset\n",
    "        \n",
    "        subset_predicted_labels = []  # Initialize list to store predicted labels for the subset\n",
    "        \n",
    "        # Determine predicted labels for each row in the subset\n",
    "        for row_results in subset_results:\n",
    "            predicted_label = 1 if row_results[0] > 0.5 else 0  # Assuming threshold of 0.5\n",
    "            subset_predicted_labels.append(predicted_label)\n",
    "        \n",
    "        subset_accuracy = accuracy_score(true_labels, subset_predicted_labels)  # Calculate accuracy score for the subset\n",
    "        accuracy_per_subset.append(round(subset_accuracy * 100, 2))  # Append accuracy score to the list\n",
    "\n",
    "    # Print accuracy scores if required\n",
    "    if (should_print):\n",
    "        for subset_index, subset_accuracy in enumerate(accuracy_per_subset):\n",
    "            print(\"Subset\", subset_index+1, \"accuracy:\", subset_accuracy)\n",
    "        \n",
    "    return accuracy_per_subset  # Return list of accuracy scores for each subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleImputer with mean strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(imputer_subsets, 'simple')\n",
    "imputer_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer_score = get_scoring(imputer_subsets, 'simple', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer_accuracy = get_accuracy(imputer_subsets, 'simple', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['simple_imputer'] = {'score': simple_imputer_score, 'accuracy': np.round(simple_imputer_accuracy, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create Gaussian Mixture Model with a single component\n",
    "gmm = GaussianMixture(n_components=1, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(multivariate_subsets, 'multivariate', default_number_of_samples, gmm)\n",
    "\n",
    "multivariate_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_score = get_scoring(multivariate_subsets, 'multivariate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_accuracy = get_accuracy(multivariate_subsets, 'multivariate', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['multivariate'] = {'score': multivariate_score, 'accuracy': np.round(multivariate_accuracy * 100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_bic(data, n_components_range):\n",
    "    \"\"\"\n",
    "    Computes the Bayesian Information Criterion (BIC) for Gaussian Mixture Models\n",
    "    with different numbers of components.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): Input data.\n",
    "        n_components_range (range): Range of number of components to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        list: BIC values for each number of components.\n",
    "    \"\"\"\n",
    "    # List to store BIC values\n",
    "    bic = []\n",
    "    \n",
    "    # Loop through number of components and compute BIC for each\n",
    "    for n_components in n_components_range:\n",
    "        # Create Gaussian Mixture Model with specified number of components\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        gmm.fit(data)  # Fit the model to the data\n",
    "        bic.append(gmm.bic(data))  # Calculate BIC and add to list\n",
    "    \n",
    "    return bic  # Return list of BIC values\n",
    "\n",
    "# Range of number of components to evaluate\n",
    "n_components_range = range(1, 51)\n",
    "\n",
    "# Compute BIC values\n",
    "bic_values = compute_bic(X_train, n_components_range)\n",
    "\n",
    "# Optimal number of components\n",
    "optimal_n_components = n_components_range[np.argmin(bic_values)]\n",
    "\n",
    "# Plotting BIC values\n",
    "plt.plot(n_components_range, bic_values, marker='o', label='BIC Values')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('BIC Value')\n",
    "plt.title('BIC for Gaussian Mixture Models')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "plt.savefig('../images/without_missingness/BIC.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Mixture Model with optimal number of components\n",
    "gmm = GaussianMixture(n_components=optimal_n_components, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(cgmm_subsets, 'cgmm', default_number_of_samples, gmm)\n",
    "\n",
    "cgmm_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_score = get_scoring(cgmm_subsets, 'cgmm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_accuracy = get_accuracy(cgmm_subsets, 'cgmm', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['cgmm'] = {'score': cgmm_score, 'accuracy': np.round(cgmm_accuracy * 100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Perform Singular Value Decomposition (SVD) on training data\n",
    "svd = TruncatedSVD(n_components=min(X_train.shape), random_state=RANDOM_STATE)\n",
    "svd.fit(X_train)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Choose threshold to preserve 90% of total variance\n",
    "threshold_index = np.argmax(cumulative_variance_ratio >= 0.90)\n",
    "threshold = svd.singular_values_[threshold_index]\n",
    "\n",
    "print(f\"Starting threshold to preserve 90% of total variance: {threshold}\")\n",
    "\n",
    "# Analyze singular values\n",
    "singular_values = svd.singular_values_\n",
    "num_non_trivial = np.sum(singular_values > threshold)  # Choose a threshold to determine non-trivial singular values\n",
    "\n",
    "# Select latent space dimensionality\n",
    "latent_dim = num_non_trivial\n",
    "\n",
    "print(f\"Number of non-trivial singular values: {num_non_trivial}\")\n",
    "print(f\"Selected latent space dimensionality: {latent_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "inputs = Input(shape=(input_dim,))\n",
    "encoded = inputs\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "z_mean = Dense(latent_dim)(encoded)\n",
    "z_log_var = Dense(latent_dim)(encoded)\n",
    "\n",
    "# Reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Define the decoder\n",
    "decoded = z\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "outputs = Dense(input_dim)(decoded)\n",
    "\n",
    "# Create the VAE model\n",
    "vae = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "vae.compile(optimizer='adam', loss='mse')  # Use MSE as the reconstruction loss\n",
    "\n",
    "# Train the model\n",
    "history = vae.fit(X_train, X_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(vae_subsets, 'vae', default_number_of_samples, vae)\n",
    "\n",
    "vae_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_score = get_scoring(vae_subsets, 'vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_accuracy = get_accuracy(vae_subsets, 'vae', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['vae'] = {'score': vae_score, 'accuracy': np.round(vae_accuracy * 100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Imputation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = load('..\\helpers\\generative_models\\cardio_gain_generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(gain_subsets, 'gain', default_number_of_samples, gain)\n",
    "\n",
    "gain_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_score = get_scoring(gain_subsets, 'gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_accuracy = get_accuracy(gain_subsets, 'gain', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['gain'] = {'score': gain_score, 'accuracy': np.round(gain_accuracy * 100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define the directory where results are stored\n",
    "results_directory = '..\\\\results\\\\without_missingness'\n",
    "\n",
    "# Get the list of existing result files to determine the next run number\n",
    "existing_files = os.listdir(results_directory)\n",
    "run_numbers = [int(file.split(\"_\")[1].split(\".\")[0]) for file in existing_files if file.startswith(\"run_\")]\n",
    "\n",
    "# Determine the next run number\n",
    "next_run_number = max(run_numbers, default=0) + 1\n",
    "\n",
    "# Create a table for accuracy\n",
    "accuracy_table = [[\"\"] + list(results_dict.keys())]\n",
    "for i in range(len(next(iter(results_dict.values()))[\"accuracy\"])):\n",
    "    accuracy_table.append([i+1] + [results_dict[key][\"accuracy\"][i] for key in results_dict.keys()])\n",
    "\n",
    "# Generate the tabulated string for accuracy\n",
    "tabulated_accuracy_table = tabulate(accuracy_table, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "\n",
    "# Create a table for score\n",
    "score_table = [[\"\"] + list(results_dict.keys())]\n",
    "for i in range(len(next(iter(results_dict.values()))[\"score\"])):\n",
    "    score_table.append([i+1] + [results_dict[key][\"score\"].get(i, \"\") for key in results_dict.keys()])\n",
    "\n",
    "# Generate the tabulated string for score\n",
    "tabulated_score_table = tabulate(score_table, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "\n",
    "print(tabulated_score_table)\n",
    "\n",
    "# Define the file name for the new result\n",
    "new_file_name = f\"run_{next_run_number}.txt\"\n",
    "file_path = os.path.join(results_directory, new_file_name)\n",
    "\n",
    "# Save both tables to the same file with separation\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"Number of datapoints: \" + str(X_train.shape[0] * fraction_of_data) + \"\\n\" + \"Number of samples: \" + str(default_number_of_samples) + \"\\n\\n\" + tabulated_accuracy_table + \"\\n\\n\\n\" + tabulated_score_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
