{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to perform a validation of methods used for multiple missing data imputation on a Cardiovascular Disease dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "module_path = os.path.abspath(os.getcwd() + '\\\\..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "RANDOM_STATE = 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1  20228       1     156    85.0    140     90            3     1      0   \n",
       "2  18857       1     165    64.0    130     70            3     1      0   \n",
       "3  17623       2     169    82.0    150    100            1     1      0   \n",
       "4  17474       1     156    56.0    100     60            1     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  \n",
       "2     0       0       1  \n",
       "3     0       1       1  \n",
       "4     0       0       0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cardio_train.csv', delimiter=';')\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['cardio'])\n",
    "y = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the results after each method\n",
    "results_dict = {}\n",
    "\n",
    "# Define the number of samples and the fraction of data to use\n",
    "default_number_of_samples = 100\n",
    "fraction_of_data = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.436062</td>\n",
       "      <td>2</td>\n",
       "      <td>0.443452</td>\n",
       "      <td>-0.847873</td>\n",
       "      <td>-0.122182</td>\n",
       "      <td>-0.088238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.307686</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.018168</td>\n",
       "      <td>0.749831</td>\n",
       "      <td>0.072610</td>\n",
       "      <td>-0.035180</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.247997</td>\n",
       "      <td>1</td>\n",
       "      <td>0.078047</td>\n",
       "      <td>-0.708942</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.748152</td>\n",
       "      <td>2</td>\n",
       "      <td>0.565254</td>\n",
       "      <td>0.541435</td>\n",
       "      <td>0.137541</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.808543</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.018168</td>\n",
       "      <td>-1.264666</td>\n",
       "      <td>-0.187113</td>\n",
       "      <td>-0.194356</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  gender    height    weight     ap_hi     ap_lo  cholesterol  \\\n",
       "0 -0.436062       2  0.443452 -0.847873 -0.122182 -0.088238            1   \n",
       "1  0.307686       1 -1.018168  0.749831  0.072610 -0.035180            3   \n",
       "2 -0.247997       1  0.078047 -0.708942  0.007679 -0.141297            3   \n",
       "3 -0.748152       2  0.565254  0.541435  0.137541  0.017879            1   \n",
       "4 -0.808543       1 -1.018168 -1.264666 -0.187113 -0.194356            1   \n",
       "\n",
       "   gluc  smoke  alco  active  \n",
       "0     1      0     0       1  \n",
       "1     1      0     0       1  \n",
       "2     1      0     0       0  \n",
       "3     1      0     0       1  \n",
       "4     1      0     0       0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Select columns to be scaled\n",
    "numeric_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
    "categorical_columns = ['gluc', 'smoke', 'alco', 'active', 'cholesterol', 'gender']\n",
    "\n",
    "# Initialize the scalers\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform your data\n",
    "X[numeric_columns] = standard_scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.272227e-16</td>\n",
       "      <td>1.349571</td>\n",
       "      <td>1.450116e-15</td>\n",
       "      <td>-2.905105e-16</td>\n",
       "      <td>7.623108e-17</td>\n",
       "      <td>1.745905e-17</td>\n",
       "      <td>1.366871</td>\n",
       "      <td>1.226457</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>0.053771</td>\n",
       "      <td>0.803729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>0.476838</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>0.680250</td>\n",
       "      <td>0.572270</td>\n",
       "      <td>0.283484</td>\n",
       "      <td>0.225568</td>\n",
       "      <td>0.397179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.514407e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.332014e+01</td>\n",
       "      <td>-4.460075e+00</td>\n",
       "      <td>-1.810381e+00</td>\n",
       "      <td>-8.841161e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.315341e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-6.527630e-01</td>\n",
       "      <td>-6.394770e-01</td>\n",
       "      <td>-5.725127e-02</td>\n",
       "      <td>-8.823850e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.489744e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.804703e-02</td>\n",
       "      <td>-1.532192e-01</td>\n",
       "      <td>-5.725127e-02</td>\n",
       "      <td>-8.823850e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.531244e-01</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.870554e-01</td>\n",
       "      <td>5.414349e-01</td>\n",
       "      <td>7.261016e-02</td>\n",
       "      <td>-3.517999e-02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.720199e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.043119e+01</td>\n",
       "      <td>8.738353e+00</td>\n",
       "      <td>1.031826e+02</td>\n",
       "      <td>5.785165e+01</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        gender        height        weight         ap_hi  \\\n",
       "count  7.000000e+04  70000.000000  7.000000e+04  7.000000e+04  7.000000e+04   \n",
       "mean   5.272227e-16      1.349571  1.450116e-15 -2.905105e-16  7.623108e-17   \n",
       "std    1.000007e+00      0.476838  1.000007e+00  1.000007e+00  1.000007e+00   \n",
       "min   -3.514407e+00      1.000000 -1.332014e+01 -4.460075e+00 -1.810381e+00   \n",
       "25%   -7.315341e-01      1.000000 -6.527630e-01 -6.394770e-01 -5.725127e-02   \n",
       "50%    9.489744e-02      1.000000  7.804703e-02 -1.532192e-01 -5.725127e-02   \n",
       "75%    7.531244e-01      2.000000  6.870554e-01  5.414349e-01  7.261016e-02   \n",
       "max    1.720199e+00      2.000000  1.043119e+01  8.738353e+00  1.031826e+02   \n",
       "\n",
       "              ap_lo   cholesterol          gluc         smoke          alco  \\\n",
       "count  7.000000e+04  70000.000000  70000.000000  70000.000000  70000.000000   \n",
       "mean   1.745905e-17      1.366871      1.226457      0.088129      0.053771   \n",
       "std    1.000007e+00      0.680250      0.572270      0.283484      0.225568   \n",
       "min   -8.841161e-01      1.000000      1.000000      0.000000      0.000000   \n",
       "25%   -8.823850e-02      1.000000      1.000000      0.000000      0.000000   \n",
       "50%   -8.823850e-02      1.000000      1.000000      0.000000      0.000000   \n",
       "75%   -3.517999e-02      2.000000      1.000000      0.000000      0.000000   \n",
       "max    5.785165e+01      3.000000      3.000000      1.000000      1.000000   \n",
       "\n",
       "             active  \n",
       "count  70000.000000  \n",
       "mean       0.803729  \n",
       "std        0.397179  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if the file exists\n",
    "file_path = \"../images/without_missingness/density_plots.png\"\n",
    "if not os.path.isfile(file_path):\n",
    "    # Set the size of the figure\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Loop through each column in X and generate a density plot\n",
    "    for i, feature_name in enumerate(X.columns):\n",
    "        # Set the subplot and plot the density of the column\n",
    "        plt.subplot(4, 4, i + 1)  # 4x4 grid, current subplot index\n",
    "        X[feature_name].plot(kind='density', color='blue', label=feature_name)\n",
    "        plt.title(feature_name)\n",
    "        plt.xlabel('Scaled Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    # Adjust the layout of the subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure as an image\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Check if the file exists\n",
    "file_path = \"../images/without_missingness/correlation_matrix.png\"\n",
    "if not os.path.isfile(file_path):\n",
    "    # Set the size of the figure\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Draw correlation matrix\n",
    "    sns.heatmap(X.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.title('Correlation Matrix')\n",
    "\n",
    "    # Save the figure as an image\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56000, 11), (14000, 11))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing 10 subsets with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets with 140 datapoints and their columns with missing values:\n",
      "Subset 1: ap_hi\n",
      "Subset 2: gluc\n",
      "Subset 3: gender\n",
      "Subset 4: ap_lo, gluc\n",
      "Subset 5: gluc, smoke\n",
      "Subset 6: weight, smoke\n",
      "Subset 7: weight, cholesterol, gluc\n",
      "Subset 8: age, gender, ap_lo\n",
      "Subset 9: ap_hi, gluc, smoke\n",
      "Subset 10: weight, ap_hi, cholesterol, alco\n"
     ]
    }
   ],
   "source": [
    "def remove_features(num_features_to_remove=None, feature_indices_to_remove=None):\n",
    "    \"\"\"\n",
    "    Randomly removes features from a subset of data and replaces their values with NaN.\n",
    "    \n",
    "    Parameters:\n",
    "        num_features_to_remove (int): Number of features to remove randomly.\n",
    "        feature_indices_to_remove (array-like): Indices of features to remove.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Subset of data with removed features and NaN values.\n",
    "    \"\"\"\n",
    "    # Sample a subset of data\n",
    "    subset = X_test.sample(frac=fraction_of_data, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Determine features to remove based on number or indices provided\n",
    "    if feature_indices_to_remove is None:\n",
    "        if num_features_to_remove is None:\n",
    "            num_features_to_remove = np.random.randint(1, min(5, len(X_test.columns) - 1))\n",
    "        else:\n",
    "            features_to_remove = np.random.choice(subset.columns[:-1], num_features_to_remove, replace=False)\n",
    "    else:\n",
    "        features_to_remove = subset.columns[feature_indices_to_remove]\n",
    "    \n",
    "    # Replace values of selected features with NaN\n",
    "    for feature in features_to_remove:\n",
    "        subset[feature] = np.NaN\n",
    "    \n",
    "    return subset.astype('object')\n",
    "\n",
    "list_of_subsets = []\n",
    "subset_without_changes = X_test.sample(frac=fraction_of_data, random_state=RANDOM_STATE)\n",
    "\n",
    "# Generate subsets with varying numbers of removed features\n",
    "list_of_subsets.append(remove_features(1, [4]))\n",
    "\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(1))\n",
    "\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(2))\n",
    "\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(3))\n",
    "\n",
    "for _ in range(1):\n",
    "    list_of_subsets.append(remove_features(4))\n",
    "\n",
    "# Print information about subsets and their missing columns\n",
    "print(f'Subsets with {list_of_subsets[0].shape[0]} datapoints and their columns with missing values:')\n",
    "for subset_index, current_row in enumerate(list_of_subsets):\n",
    "    nan_columns = current_row.columns[current_row.isnull().all()]\n",
    "    print(f\"Subset {subset_index+1}: {', '.join(nan_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def simple_impute(current_subset):\n",
    "    \"\"\"\n",
    "    Impute missing values using SimpleImputer with mean strategy.\n",
    "\n",
    "    Parameters:\n",
    "        current_subset (pandas.DataFrame): Subset of data with missing values.\n",
    "\n",
    "    This function iterates over each column in the given DataFrame and imputes missing values using\n",
    "    the SimpleImputer class from scikit-learn. The imputer is initialized with the 'mean' strategy,\n",
    "    which replaces missing values with the mean of the non-missing values in the column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a SimpleImputer object with 'mean' strategy\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in current_subset.columns:\n",
    "        # Check if any values in the column are missing\n",
    "        if pd.isna(current_subset[col]).any():\n",
    "            # Fit the imputer to the non-missing values in the column\n",
    "            imp.fit(subset_without_changes[[col]])\n",
    "            # Transform the missing values in the column\n",
    "            current_subset[col] = imp.transform(current_subset[[col]])\n",
    "            \n",
    "            # if col in categorical_columns:\n",
    "            #     # Approximate categorical values to the nearest whole number\n",
    "            #     current_subset[col] = np.round(current_subset[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate normal distribution & cGMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.ConditionalGMM.condGMM import CondGMM\n",
    "\n",
    "def cgmm_impute(gmm, missing_features_indices, current_row, number_of_samples):\n",
    "    \"\"\"\n",
    "    Impute missing values using Conditional GMM, returning parameters of the predictive distribution.\n",
    "    \n",
    "    Parameters:\n",
    "        gmm (GMM): Gaussian Mixture Model.\n",
    "        missing_features_indices (list): Indices of missing features.\n",
    "        current_row (pandas.Series): Current row with missing values.\n",
    "        number_of_samples (int): Number of samples to generate.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing samples, means ('mu'), and covariances ('sigma').\n",
    "    \"\"\"\n",
    "    # Find indices of known features\n",
    "    known_features_indices = [i for i in range(len(current_row)) if i not in missing_features_indices]\n",
    "    \n",
    "    # Extract values of known features for the given row\n",
    "    known_features_values = current_row.iloc[known_features_indices].values\n",
    "    \n",
    "    # Initialize CondGMM\n",
    "    cGMM = CondGMM(gmm.weights_, gmm.means_, gmm.covariances_, known_features_indices)\n",
    "    \n",
    "    # Generate samples using Conditional GMM\n",
    "    generated_samples = cGMM.rvs(known_features_values, size=number_of_samples, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Make sure that the generated samples do not contain NaN values, if it does then change it's value to 0\n",
    "    generated_samples[np.isnan(generated_samples)] = 0\n",
    "    \n",
    "    # Extract mean and covariance for the conditional distribution\n",
    "    mus = cGMM.conditional_component_means(known_features_values)\n",
    "    c_weights = cGMM.conditional_weights(known_features_values)\n",
    "    c_weights = c_weights[:, np.newaxis]  # Ensure weights are aligned for broadcasting\n",
    "    mu = np.sum(c_weights * mus, axis=0)  # Weighted sum across the correct axis\n",
    "    sigma = cGMM.conditional_component_covs()\n",
    "\n",
    "    return {\n",
    "        \"samples\": generated_samples,\n",
    "        \"mu\": mu,\n",
    "        \"sigma\": sigma\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAEAC & GAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaeac_gain_impute(model, missing_features_indices, current_row, number_of_samples):\n",
    "    \"\"\"\n",
    "    Impute missing values using a Variational Autoencoder or Generative Adversarial Imputation Network.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Trained Variational Autoencoder or GAIN model.\n",
    "        missing_features_indices (list): Indices of missing features.\n",
    "        current_row (pandas.Series): Current row with missing values.\n",
    "        number_of_samples (int): Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: Generated data with imputed missing values.\n",
    "    \"\"\"\n",
    "    generated_samples = np.empty((number_of_samples, len(missing_features_indices)))\n",
    "    \n",
    "    # Repeat the prediction process for the specified number of samples\n",
    "    for i in range(number_of_samples):\n",
    "        # Impute missing values for each feature index\n",
    "        # Reshape the missing values to a 2D array with one row and all missing features\n",
    "        missing_features_values = model.predict(current_row.values.reshape(1, -1).astype(np.float32), verbose=0)\n",
    "        \n",
    "        # Store the generated data in the array\n",
    "        generated_samples[i] = missing_features_values[:, missing_features_indices]\n",
    "    \n",
    "    return {\n",
    "        \"samples\": generated_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute multiple missing data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def imputing_missing_data(subsets, method='simple', number_of_samples=default_number_of_samples, model=None):\n",
    "    \"\"\"\n",
    "    Impute missing data in subsets using different imputation methods.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Imputation method ('simple', 'multivariate', 'cgmm', 'vaeac', or 'gain').\n",
    "        model: Trained model for certain imputation methods.\n",
    "    \"\"\"\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        if method == 'simple':\n",
    "            # Simple Imputer\n",
    "            generated_data = simple_impute(subset)\n",
    "        else:\n",
    "            # Initialize to keep track of actual row index, because indices were shuffled\n",
    "            row_in_subset_index = 0\n",
    "            \n",
    "            for row_index, row in subset.iterrows():\n",
    "                # Get indices of unknown features\n",
    "                missing_features_indices = [row.index.get_loc(col) for col in row.index if pd.isna(row[col])]\n",
    "                \n",
    "                # If all features are known, continue   \n",
    "                if len(missing_features_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                generated_data = None\n",
    "                \n",
    "                if method == 'multivariate' or method == 'cgmm':\n",
    "                    # Multivariate Imputer or Conditional GMM\n",
    "                    generated_data = cgmm_impute(model, missing_features_indices, row, number_of_samples)\n",
    "                elif method == 'vaeac' or method == 'gain':\n",
    "                    # Variational AutoEncoder or Generative Adversarial Imputation Network\n",
    "                    generated_data = vaeac_gain_impute(model, missing_features_indices, row, number_of_samples)\n",
    "                    \n",
    "                # Update unknown features with sampled data\n",
    "                for feature_index in range(len(missing_features_indices)):\n",
    "                    # Check if generated_data is a dictionary\n",
    "                    if isinstance(generated_data, dict):\n",
    "                        if 'mu' in generated_data and 'sigma' in generated_data:\n",
    "                            # Convert mu and sigma to lists if they are numpy arrays\n",
    "                            mu = generated_data['mu'].tolist() if isinstance(generated_data['mu'], np.ndarray) else generated_data['mu']\n",
    "                            sigma = generated_data['sigma'].tolist() if isinstance(generated_data['sigma'], np.ndarray) else generated_data['sigma']\n",
    "                            samples = [sample[feature_index] for sample in generated_data['samples']]\n",
    "                            \n",
    "                            data_to_insert = json.dumps({\n",
    "                                \"samples\": samples,\n",
    "                                \"mu\": mu,\n",
    "                                \"sigma\": sigma\n",
    "                            })\n",
    "                        else:\n",
    "                            samples = [sample[feature_index] for sample in generated_data['samples']]\n",
    "                            data_to_insert = json.dumps({\n",
    "                                \"samples\": samples\n",
    "                            })\n",
    "                        \n",
    "                        subset.at[row_index, subset.columns[missing_features_indices[feature_index]]] = data_to_insert\n",
    "                \n",
    "                row_in_subset_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from properscoring import crps_ensemble\n",
    "\n",
    "def get_scoring(subsets, method='simple', print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate scores (NMSE, Log Score, and CRPS) for features in subsets of data.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Imputation method ('simple', 'multivariate', 'cgmm', 'vaeac', or 'gain').\n",
    "        print_results (bool): Whether to print scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing scores for each feature in the subsets organized by score type.\n",
    "    \"\"\"\n",
    "    method = method.lower()\n",
    "\n",
    "    # Deserialize any strings in subsets\n",
    "    for subset in subsets:\n",
    "        for col in subset.columns:\n",
    "            subset[col] = subset[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    all_subsets_scores = {}\n",
    "\n",
    "    # TODO: confirm that the order of missing features is correct\n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        feature_scores = {}  # Dictionary for each type of score per feature\n",
    "        \n",
    "        # Identify features with missing values\n",
    "        if method == 'simple':\n",
    "            missing_features_indices = [col_index for col_index, col in enumerate(list_of_subsets[subset_index].columns) if list_of_subsets[subset_index][col].isnull().all()]\n",
    "        else:\n",
    "            missing_features_indices = [col_index for col_index, col in enumerate(subset.columns) if subset[col].apply(lambda x: isinstance(x, (list, dict))).any()]\n",
    "\n",
    "        if not missing_features_indices:\n",
    "            continue  # Skip if no missing values\n",
    "        \n",
    "        for row_index, row in subset.iterrows():\n",
    "            original_values = X.iloc[row_index, missing_features_indices].values\n",
    "            \n",
    "            for feature_index, col_index in enumerate(missing_features_indices):\n",
    "                feature_name = subset.columns[col_index]\n",
    "                generated_samples = row.iloc[col_index]\n",
    "                original_value = original_values[feature_index]\n",
    "                \n",
    "                # Initialize dictionary only with NMSE, others will be added as needed\n",
    "                if feature_name not in feature_scores:\n",
    "                    feature_scores[feature_name] = {'nmse': []}\n",
    "                \n",
    "                if not isinstance(generated_samples, dict):\n",
    "                    generated_samples = {'samples': [generated_samples]}\n",
    "\n",
    "                if 'mu' in generated_samples and 'sigma' in generated_samples:\n",
    "                    epsilon = 1e-10\n",
    "                    \n",
    "                    # Read mean and covariance\n",
    "                    mu = np.mean(generated_samples['mu']) if isinstance(generated_samples['mu'], (list, np.ndarray)) else generated_samples['mu']\n",
    "                    sigma = np.mean(generated_samples['sigma']) if isinstance(generated_samples['sigma'], (list, np.ndarray)) else generated_samples['sigma']\n",
    "                    sigma = max(sigma, epsilon)  # Ensure sigma is positive\n",
    "                    \n",
    "                    # Log score calculation\n",
    "                    log_score = -np.log(norm.pdf(original_value, loc=mu, scale=sigma) + epsilon)\n",
    "                    feature_scores[feature_name].setdefault('log_score', []).append(log_score)\n",
    "                \n",
    "                elif 'samples' in generated_samples:\n",
    "                    # CRPS for ensemble predictions\n",
    "                    samples_size = len(generated_samples['samples'])\n",
    "                    original_value_array = np.repeat(original_value, samples_size)\n",
    "                    crps_score = crps_ensemble(original_value_array, generated_samples['samples'])\n",
    "                    feature_scores[feature_name].setdefault('crps', []).append(crps_score)\n",
    "                \n",
    "                # MSE calculation\n",
    "                squared_errors = [(original_value - x)**2 for x in generated_samples['samples']]\n",
    "                feature_scores[feature_name]['nmse'].append(squared_errors)\n",
    "        \n",
    "        # Average scores for each feature\n",
    "        for feature_name, scores in feature_scores.items():\n",
    "            if feature_name == 'ap_hi':\n",
    "                print(\"ap_hi\")\n",
    "            for score_type, values in scores.items():\n",
    "                if values:\n",
    "                    mean_score = np.mean(values)\n",
    "                    \n",
    "                    if score_type == 'nmse':\n",
    "                        variance = np.var(subset_without_changes[feature_name])\n",
    "                        \n",
    "                        if variance == 0:\n",
    "                            if mean_score != 0:\n",
    "                                raise Exception(\"Mean Squared Error cannot be different than 0 when variance is equal to 0!\")\n",
    "                            else:\n",
    "                                mean_score = 0\n",
    "                        else:\n",
    "                            mean_score = mean_score / variance\n",
    "                            \n",
    "                    feature_scores[feature_name][score_type] = np.round(mean_score, 3)\n",
    "        \n",
    "        all_subsets_scores[subset_index] = feature_scores\n",
    "        \n",
    "        # Print scores if required\n",
    "        if print_results:\n",
    "            print(f\"Scores for Subset {subset_index + 1}:\")\n",
    "            for feature_name, scores in feature_scores.items():\n",
    "                print(f\"Feature {feature_name}: \", end=\"\")\n",
    "                for score_type, score_value in scores.items():\n",
    "                    print(f\"{score_type.upper()} = {score_value}, \", end=\"\")\n",
    "                print()  # New line for each feature\n",
    "            \n",
    "    return all_subsets_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message=\"X does not have valid feature names\")\n",
    "\n",
    "# Load the trained classifier model\n",
    "classifier = load('..\\helpers\\predictive_models\\cardio_classifier.h5')\n",
    "\n",
    "def get_classification_result(subsets, method='simple', should_print=False):\n",
    "    \"\"\"\n",
    "    Calculate AUC scores for subsets of data using a trained classifier.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Imputation method ('simple', 'multivariate', 'cgmm', 'vaeac', or 'gain').\n",
    "        should_print (bool): Whether to print AUC scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of AUC scores for each subset.\n",
    "    \"\"\"\n",
    "    method = method.lower()\n",
    "    \n",
    "    # Deserialize any strings in subsets\n",
    "    for subset in subsets:\n",
    "        for col_index in subset.columns:\n",
    "            subset[col_index] = subset[col_index].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    classification_results = []  # Initialize list to store classification results\n",
    "    accuracy_per_subset = []  # Initialize list to store accuracy scores\n",
    "    auc_per_subset = []  # Initialize list to store AUC scores\n",
    "\n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        subset_results = []  # Initialize list to store results for the current subset\n",
    "        \n",
    "        # Iterate through rows in the subset DataFrame\n",
    "        for row_index, row in subset.iterrows():\n",
    "            output_probs = []  # Initialize list to store results for the current row\n",
    "            \n",
    "            # Process each row based on the method used\n",
    "            if method != 'simple':\n",
    "                serialized_arrays = []\n",
    "                non_serialized_values = []\n",
    "                \n",
    "                # Split row values into serialized arrays and non-serialized values\n",
    "                for col_index, value in enumerate(row):\n",
    "                    if isinstance(value, dict):\n",
    "                        serialized_arrays.append((col_index, value['samples']))\n",
    "                    else:\n",
    "                        non_serialized_values.append((col_index, value))\n",
    "                \n",
    "                # Generate combined rows by combining serialized arrays with non-serialized values\n",
    "                for i in range(default_number_of_samples):\n",
    "                    combined_row = non_serialized_values.copy()\n",
    "                    \n",
    "                    for col_index, serialized_array in serialized_arrays:\n",
    "                        assert len(serialized_array) == default_number_of_samples\n",
    "                        combined_row.append((col_index, serialized_array[i]))\n",
    "                    \n",
    "                    # TODO: confirm that combined_row_array comes out in proper order\n",
    "                    combined_row_array = np.zeros(shape=len(combined_row))\n",
    "                    for col_index, value in combined_row:\n",
    "                        combined_row_array[col_index] = value\n",
    "                    \n",
    "                    output_probs.append(combined_row_array)\n",
    "            else:\n",
    "                output_probs.append(row.values.tolist())\n",
    "            \n",
    "            predicted_probs = classifier.predict_proba(np.vstack(output_probs))\n",
    "                \n",
    "            subset_results.append(predicted_probs)\n",
    "        \n",
    "        classification_results.append(subset_results)\n",
    "\n",
    "    # Create an empty list to store dictionaries of results\n",
    "    results_list = []\n",
    "\n",
    "    # Calculate AUC scores and accuracy for each subset\n",
    "    for subset_index, subset_results in enumerate(classification_results):\n",
    "        true_labels = y.loc[subsets[subset_index].index]\n",
    "        \n",
    "        subset_predicted_probs = []  # Initialize list to store predicted probabilities for the subset\n",
    "        \n",
    "        # Determine predicted probabilities for each row in the subset\n",
    "        for output_probs in subset_results:\n",
    "            predicted_prob = np.mean(output_probs[:, 1])  # Assuming the second column contains probabilities of the positive class\n",
    "            subset_predicted_probs.append(predicted_prob)\n",
    "        \n",
    "        # Convert probabilities to binary predictions based on the threshold\n",
    "        subset_predictions = [1 if prob > 0.5 else 0 for prob in subset_predicted_probs]\n",
    "        \n",
    "        subset_accuracy = accuracy_score(true_labels, subset_predictions)  # Calculate accuracy score for the subset\n",
    "        subset_auc = roc_auc_score(true_labels, subset_predicted_probs)  # Calculate AUC score for the subset\n",
    "        \n",
    "        accuracy_per_subset.append(round(subset_accuracy, 2))  # Append accuracy score to the list\n",
    "        auc_per_subset.append(round(subset_auc, 2))  # Append AUC score to the list\n",
    "        \n",
    "        # Append results to the list of dictionaries\n",
    "        results_list.append({'Accuracy': np.round(subset_accuracy * 100, 2), 'AUC': np.round(subset_auc, 3)})\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    results_table = pd.DataFrame(results_list)\n",
    "\n",
    "    # Print classification scores in a table\n",
    "    if should_print:\n",
    "        print(results_table)\n",
    "        \n",
    "    return accuracy_per_subset, auc_per_subset  # Return list of AUC scores for each subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleImputer with mean strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>1.164922</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.87078</td>\n",
       "      <td>-1.125735</td>\n",
       "      <td>-0.017783</td>\n",
       "      <td>-0.088238</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>-1.175756</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.530961</td>\n",
       "      <td>0.194108</td>\n",
       "      <td>-0.017783</td>\n",
       "      <td>-0.03518</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53089</th>\n",
       "      <td>0.360782</td>\n",
       "      <td>2</td>\n",
       "      <td>0.930659</td>\n",
       "      <td>-0.014288</td>\n",
       "      <td>-0.017783</td>\n",
       "      <td>-0.03518</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17450</th>\n",
       "      <td>-1.643486</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.40916</td>\n",
       "      <td>1.444485</td>\n",
       "      <td>-0.017783</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24702</th>\n",
       "      <td>1.599416</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.530961</td>\n",
       "      <td>2.555932</td>\n",
       "      <td>-0.017783</td>\n",
       "      <td>-0.03518</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age gender    height    weight     ap_hi     ap_lo cholesterol  \\\n",
       "2258   1.164922      1  -1.87078 -1.125735 -0.017783 -0.088238           2   \n",
       "24996 -1.175756      1 -0.530961  0.194108 -0.017783  -0.03518           1   \n",
       "53089  0.360782      2  0.930659 -0.014288 -0.017783  -0.03518           1   \n",
       "17450 -1.643486      1  -0.40916  1.444485 -0.017783 -0.141297           1   \n",
       "24702  1.599416      1 -0.530961  2.555932 -0.017783  -0.03518           1   \n",
       "\n",
       "      gluc smoke alco active  \n",
       "2258     3     0    0      1  \n",
       "24996    1     0    0      0  \n",
       "53089    1     0    0      1  \n",
       "17450    1     0    1      1  \n",
       "24702    1     0    0      1  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputing_missing_data(imputer_subsets, 'simple')\n",
    "\n",
    "imputer_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap_hi\n",
      "ap_hi\n",
      "ap_hi\n"
     ]
    }
   ],
   "source": [
    "simple_imputer_score = get_scoring(imputer_subsets, 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy    AUC\n",
      "0     74.29  0.762\n",
      "1     76.43  0.824\n",
      "2     75.71  0.833\n",
      "3     73.57  0.828\n",
      "4     76.43  0.818\n",
      "5     76.43  0.812\n",
      "6     77.86  0.820\n",
      "7     74.29  0.802\n",
      "8     70.71  0.740\n",
      "9     62.86  0.705\n"
     ]
    }
   ],
   "source": [
    "simple_imputer_accuracy, simple_imputer_auc = get_classification_result(imputer_subsets, 'simple', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['simple_imputer'] = {'score': simple_imputer_score, 'accuracy': simple_imputer_accuracy, 'auc': simple_imputer_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(random_state=404)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GaussianMixture<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.mixture.GaussianMixture.html\">?<span>Documentation for GaussianMixture</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GaussianMixture(random_state=404)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "GaussianMixture(random_state=404)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create Gaussian Mixture Model with a single component\n",
    "gmm = GaussianMixture(n_components=1, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>1.164922</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.87078</td>\n",
       "      <td>-1.125735</td>\n",
       "      <td>{\"samples\": [0.8396580083006695, -1.9221773806...</td>\n",
       "      <td>-0.088238</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>-1.175756</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.530961</td>\n",
       "      <td>0.194108</td>\n",
       "      <td>{\"samples\": [0.8142443832080619, -1.9475910057...</td>\n",
       "      <td>-0.03518</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53089</th>\n",
       "      <td>0.360782</td>\n",
       "      <td>2</td>\n",
       "      <td>0.930659</td>\n",
       "      <td>-0.014288</td>\n",
       "      <td>{\"samples\": [0.8405931506134794, -1.9212422383...</td>\n",
       "      <td>-0.03518</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17450</th>\n",
       "      <td>-1.643486</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.40916</td>\n",
       "      <td>1.444485</td>\n",
       "      <td>{\"samples\": [0.840238835059227, -1.92159655390...</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24702</th>\n",
       "      <td>1.599416</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.530961</td>\n",
       "      <td>2.555932</td>\n",
       "      <td>{\"samples\": [0.9206615919666465, -1.8411737970...</td>\n",
       "      <td>-0.03518</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age gender    height    weight  \\\n",
       "2258   1.164922      1  -1.87078 -1.125735   \n",
       "24996 -1.175756      1 -0.530961  0.194108   \n",
       "53089  0.360782      2  0.930659 -0.014288   \n",
       "17450 -1.643486      1  -0.40916  1.444485   \n",
       "24702  1.599416      1 -0.530961  2.555932   \n",
       "\n",
       "                                                   ap_hi     ap_lo  \\\n",
       "2258   {\"samples\": [0.8396580083006695, -1.9221773806... -0.088238   \n",
       "24996  {\"samples\": [0.8142443832080619, -1.9475910057...  -0.03518   \n",
       "53089  {\"samples\": [0.8405931506134794, -1.9212422383...  -0.03518   \n",
       "17450  {\"samples\": [0.840238835059227, -1.92159655390... -0.141297   \n",
       "24702  {\"samples\": [0.9206615919666465, -1.8411737970...  -0.03518   \n",
       "\n",
       "      cholesterol gluc smoke alco active  \n",
       "2258            2    3     0    0      1  \n",
       "24996           1    1     0    0      0  \n",
       "53089           1    1     0    0      1  \n",
       "17450           1    1     0    1      1  \n",
       "24702           1    1     0    0      1  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputing_missing_data(multivariate_subsets, 'multivariate', default_number_of_samples, gmm)\n",
    "\n",
    "multivariate_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ap_hi\n",
      "Scores for Subset 1:\n",
      "Feature ap_hi: NMSE = 147.863, LOG_SCORE = 1.138, \n",
      "Scores for Subset 2:\n",
      "Feature gluc: NMSE = 1.397, LOG_SCORE = 1.406, \n",
      "Scores for Subset 3:\n",
      "Feature gender: NMSE = 1.552, LOG_SCORE = 2.324, \n",
      "Scores for Subset 4:\n",
      "Feature ap_lo: NMSE = 3.005, LOG_SCORE = 2.928, \n",
      "Feature gluc: NMSE = 1.279, LOG_SCORE = 3.493, \n",
      "Scores for Subset 5:\n",
      "Feature gluc: NMSE = 1.3, LOG_SCORE = 12.414, \n",
      "Feature smoke: NMSE = 1.72, LOG_SCORE = 20.818, \n",
      "Scores for Subset 6:\n",
      "Feature weight: NMSE = 1.981, LOG_SCORE = 5.41, \n",
      "Feature smoke: NMSE = 1.717, LOG_SCORE = 0.137, \n",
      "Scores for Subset 7:\n",
      "Feature weight: NMSE = 1.819, LOG_SCORE = 9.639, \n",
      "Feature cholesterol: NMSE = 1.889, LOG_SCORE = 4.131, \n",
      "Feature gluc: NMSE = 1.85, LOG_SCORE = 2.866, \n",
      "Scores for Subset 8:\n",
      "Feature age: NMSE = 1.766, LOG_SCORE = 7.524, \n",
      "Feature gender: NMSE = 1.573, LOG_SCORE = 7.904, \n",
      "Feature ap_lo: NMSE = 2.889, LOG_SCORE = 2.521, \n",
      "ap_hi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m multivariate_score \u001b[38;5;241m=\u001b[39m \u001b[43mget_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmultivariate_subsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmultivariate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 94\u001b[0m, in \u001b[0;36mget_scoring\u001b[1;34m(subsets, method, print_results)\u001b[0m\n\u001b[0;32m     92\u001b[0m                         mean_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     93\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m                     mean_score \u001b[38;5;241m=\u001b[39m \u001b[43mmean_score\u001b[49m \u001b[38;5;241m/\u001b[39m variance\n\u001b[0;32m     96\u001b[0m             feature_scores[feature_name][score_type] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(mean_score, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     98\u001b[0m all_subsets_scores[subset_index] \u001b[38;5;241m=\u001b[39m feature_scores\n",
      "Cell \u001b[1;32mIn[41], line 94\u001b[0m, in \u001b[0;36mget_scoring\u001b[1;34m(subsets, method, print_results)\u001b[0m\n\u001b[0;32m     92\u001b[0m                         mean_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     93\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m                     mean_score \u001b[38;5;241m=\u001b[39m \u001b[43mmean_score\u001b[49m \u001b[38;5;241m/\u001b[39m variance\n\u001b[0;32m     96\u001b[0m             feature_scores[feature_name][score_type] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(mean_score, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     98\u001b[0m all_subsets_scores[subset_index] \u001b[38;5;241m=\u001b[39m feature_scores\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\pieni\\Documents\\GitHub\\GenerativeMLForPrecisionMedicine\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pieni\\Documents\\GitHub\\GenerativeMLForPrecisionMedicine\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "multivariate_score = get_scoring(multivariate_subsets, 'multivariate', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_accuracy, multivariate_auc = get_classification_result(multivariate_subsets, 'multivariate', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['multivariate'] = {'score': multivariate_score, 'accuracy': multivariate_accuracy, 'auc': multivariate_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_bic(data, n_components_range):\n",
    "    \"\"\"\n",
    "    Computes the Bayesian Information Criterion (BIC) for Gaussian Mixture Models\n",
    "    with different numbers of components.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): Input data.\n",
    "        n_components_range (range): Range of number of components to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        list: BIC values for each number of components.\n",
    "    \"\"\"\n",
    "    # List to store BIC values\n",
    "    bic = []\n",
    "    \n",
    "    # Loop through number of components and compute BIC for each\n",
    "    for n_components in n_components_range:\n",
    "        # Create Gaussian Mixture Model with specified number of components\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        gmm.fit(data)  # Fit the model to the data\n",
    "        bic.append(gmm.bic(data))  # Calculate BIC and add to list\n",
    "    \n",
    "    return bic  # Return list of BIC values\n",
    "\n",
    "# Used to simplify getting optimal number of components based on previous run\n",
    "optimal_n_components = 46\n",
    "\n",
    "if optimal_n_components is None:\n",
    "    # Range of number of components to evaluate\n",
    "    n_components_range = range(1, 51)\n",
    "\n",
    "    # Compute BIC values\n",
    "    bic_values = compute_bic(X_train, n_components_range)\n",
    "\n",
    "    # Optimal number of components\n",
    "    optimal_n_components = n_components_range[np.argmin(bic_values)]\n",
    "\n",
    "    # Plotting BIC values\n",
    "    plt.plot(n_components_range, bic_values, marker='o', label='BIC Values')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('BIC Value')\n",
    "    plt.title('BIC for Gaussian Mixture Models')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    plt.savefig('../images/without_missingness/BIC.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Mixture Model with optimal number of components\n",
    "gmm = GaussianMixture(n_components=optimal_n_components, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(cgmm_subsets, 'cgmm', default_number_of_samples, gmm)\n",
    "\n",
    "cgmm_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_score = get_scoring(cgmm_subsets, 'cgmm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_accuracy, cgmm_auc = get_classification_result(cgmm_subsets, 'cgmm', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['cgmm'] = {'score': cgmm_score, 'accuracy': cgmm_accuracy, 'auc': cgmm_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder with Arbitrary Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaeac_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Perform Singular Value Decomposition (SVD) on training data\n",
    "svd = TruncatedSVD(n_components=min(X_train.shape), random_state=RANDOM_STATE)\n",
    "svd.fit(X_train)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Choose threshold to preserve 90% of total variance\n",
    "threshold_index = np.argmax(cumulative_variance_ratio >= 0.90)\n",
    "threshold = svd.singular_values_[threshold_index]\n",
    "\n",
    "print(f\"Starting threshold to preserve 90% of total variance: {threshold}\")\n",
    "\n",
    "# Analyze singular values\n",
    "singular_values = svd.singular_values_\n",
    "num_non_trivial = np.sum(singular_values > threshold)  # Choose a threshold to determine non-trivial singular values\n",
    "\n",
    "# Select latent space dimensionality\n",
    "latent_dim = num_non_trivial\n",
    "\n",
    "print(f\"Number of non-trivial singular values: {num_non_trivial}\")\n",
    "print(f\"Selected latent space dimensionality: {latent_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "inputs = Input(shape=(input_dim,))\n",
    "encoded = inputs\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "z_mean = Dense(latent_dim)(encoded)\n",
    "z_log_var = Dense(latent_dim)(encoded)\n",
    "\n",
    "# Reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Define the decoder\n",
    "decoded = z\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "outputs = Dense(input_dim)(decoded)\n",
    "\n",
    "# Create the VAE model\n",
    "vaeac = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "vaeac.compile(optimizer='adam', loss='mse')  # Use MSE as the reconstruction loss\n",
    "\n",
    "# Train the model\n",
    "history = vaeac.fit(X_train, X_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(vaeac_subsets, 'vaeac', default_number_of_samples, vaeac)\n",
    "\n",
    "vaeac_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaeac_score = get_scoring(vaeac_subsets, 'vaeac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaeac_accuracy, vaeac_auc = get_classification_result(vaeac_subsets, 'vaeac', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['vaeac'] = {'score': vaeac_score, 'accuracy': vaeac_accuracy, 'auc': vaeac_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Imputation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = load('..\\helpers\\generative_models\\cardio_gain_generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(gain_subsets, 'gain', default_number_of_samples, gain)\n",
    "\n",
    "gain_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_score = get_scoring(gain_subsets, 'gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_accuracy, gain_auc = get_classification_result(gain_subsets, 'gain', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['gain'] = {'score': gain_score, 'accuracy': gain_accuracy, 'auc': gain_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Define the directory where results are stored\n",
    "results_directory = '..\\\\results\\\\without_missingness'\n",
    "\n",
    "# Get the list of existing result files to determine the next run number\n",
    "existing_files = os.listdir(results_directory)\n",
    "run_numbers = [int(file.split(\"_\")[1].split(\".\")[0]) for file in existing_files if file.startswith(\"run_\")]\n",
    "\n",
    "# Determine the next run number\n",
    "next_run_number = max(run_numbers, default=0) + 1\n",
    "\n",
    "# Create tables for accuracy, AUC, and scores\n",
    "accuracy_table = [[\"\"] + list(results_dict.keys())]\n",
    "auc_table = [[\"\"] + list(results_dict.keys())]\n",
    "score_table = [[\"\"] + list(results_dict.keys())]\n",
    "\n",
    "for i in range(len(next(iter(results_dict.values()))[\"accuracy\"])):\n",
    "    accuracy_row = [i+1] + [results_dict[key][\"accuracy\"][i] for key in results_dict.keys()]\n",
    "    auc_row = [i+1] + [results_dict[key][\"auc\"][i] for key in results_dict.keys()]\n",
    "    score_row = [i+1] + [results_dict[key][\"score\"].get(i, \"\") for key in results_dict.keys()]\n",
    "    accuracy_table.append(accuracy_row)\n",
    "    auc_table.append(auc_row)\n",
    "    score_table.append(score_row)\n",
    "\n",
    "# Generate the tabulated strings for accuracy, AUC, and scores\n",
    "tabulated_accuracy_table = tabulate(accuracy_table, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "tabulated_auc_table = tabulate(auc_table, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "tabulated_score_table = tabulate(score_table, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "\n",
    "# Define the file name for the new result\n",
    "new_file_name = f\"run_{next_run_number}.txt\"\n",
    "file_path = os.path.join(results_directory, new_file_name)\n",
    "\n",
    "# Save accuracy, AUC, and score tables to the same file with separation\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"Number of datapoints: \" + str(X_train.shape[0] * fraction_of_data) + \"\\n\" + \"Number of samples: \" + str(default_number_of_samples) + \"\\n\\n\")\n",
    "    file.write(\"Accuracy:\\n\")\n",
    "    file.write(tabulated_accuracy_table + \"\\n\\n\")\n",
    "    file.write(\"AUC:\\n\")\n",
    "    file.write(tabulated_auc_table + \"\\n\\n\")\n",
    "    file.write(\"Scores:\\n\")\n",
    "    file.write(tabulated_score_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
