{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to perform a validation of methods used for multiple missing data imputation on a Cardiovascular Disease dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "module_path = os.path.abspath(os.getcwd() + '\\\\..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "RANDOM_STATE = 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cardio_train.csv', delimiter=';')\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['cardio'])\n",
    "y = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the results after each method\n",
    "results_dict = {}\n",
    "\n",
    "# Define the number of samples and the fraction of data to use\n",
    "default_number_of_samples = 10\n",
    "fraction_of_data = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_remove = []\n",
    "\n",
    "for index, row in X.iterrows():\n",
    "    if X['ap_hi'].iloc[index] < 55 or X['ap_hi'].iloc[index] > 200 or X['ap_lo'].iloc[index] < 55 or X['ap_lo'].iloc[index] > 120 or X['height'].iloc[index] < 125 or X['height'].iloc[index] > 200:\n",
    "        indices_to_remove.append(index)\n",
    "        \n",
    "X = X.drop(indices_to_remove)\n",
    "y = y.drop(indices_to_remove)\n",
    "\n",
    "# Reindexing\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Select columns to be scaled\n",
    "numeric_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
    "categorical_columns = ['gluc', 'smoke', 'alco', 'active', 'cholesterol', 'gender']\n",
    "\n",
    "# Initialize the scalers\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform your data\n",
    "X[numeric_columns] = standard_scaler.fit_transform(X[numeric_columns])\n",
    "X[categorical_columns] = X[categorical_columns].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if the file exists\n",
    "file_path = \"../images/without_missingness/density_plots.png\"\n",
    "if not os.path.isfile(file_path):\n",
    "    # Set the size of the figure\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Loop through each column in X and generate a density plot\n",
    "    for i, feature_name in enumerate(X.columns):\n",
    "        # Set the subplot and plot the density of the column\n",
    "        plt.subplot(4, 4, i + 1)  # 4x4 grid, current subplot index\n",
    "        X[feature_name].plot(kind='density', color='blue', label=feature_name)\n",
    "        plt.title(feature_name, fontsize=32)\n",
    "        plt.xlabel('Scaled Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    # Adjust the layout of the subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure as an image\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Check if the file exists\n",
    "file_path = \"../images/without_missingness/correlation_matrix.png\"\n",
    "if not os.path.isfile(file_path):\n",
    "    # Set the size of the figure\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Draw correlation matrix\n",
    "    sns.heatmap(X.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.title('Correlation Matrix')\n",
    "\n",
    "    # Save the figure as an image\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing 10 subsets with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(num_features_to_remove=None, feature_indices_to_remove=None):\n",
    "    \"\"\"\n",
    "    Randomly removes features from a subset of data and replaces their values with NaN.\n",
    "    \n",
    "    Parameters:\n",
    "        num_features_to_remove (int): Number of features to remove randomly.\n",
    "        feature_indices_to_remove (array-like): Indices of features to remove.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Subset of data with removed features and NaN values.\n",
    "    \"\"\"\n",
    "    # Sample a subset of data\n",
    "    subset = X_test.sample(frac=fraction_of_data, random_state=RANDOM_STATE).copy()\n",
    "    # subset = X_test.copy()\n",
    "    \n",
    "    # Determine features to remove based on number or indices provided\n",
    "    if feature_indices_to_remove is None:\n",
    "        if num_features_to_remove is None:\n",
    "            num_features_to_remove = np.random.randint(1, min(5, len(X_test.columns) - 1))\n",
    "        else:\n",
    "            features_to_remove = np.random.choice(subset.columns[:-1], num_features_to_remove, replace=False)\n",
    "    else:\n",
    "        features_to_remove = subset.columns[feature_indices_to_remove]\n",
    "    \n",
    "    # Replace values of selected features with NaN\n",
    "    for feature in features_to_remove:\n",
    "        subset[feature] = np.NaN\n",
    "    \n",
    "    return subset.astype('object')\n",
    "\n",
    "list_of_subsets = []\n",
    "subset_without_changes = X_test.sample(frac=fraction_of_data, random_state=RANDOM_STATE).copy()\n",
    "\n",
    "# Generate subsets with varying numbers of removed features\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(1))\n",
    "\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(2))\n",
    "\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(3))\n",
    "\n",
    "for _ in range(1):\n",
    "    list_of_subsets.append(remove_features(4))\n",
    "\n",
    "# Print information about subsets and their missing columns\n",
    "print(f'Subsets with {list_of_subsets[0].shape[0]} datapoints and their columns with missing values:')\n",
    "for subset_index, current_row in enumerate(list_of_subsets):\n",
    "    nan_columns = current_row.columns[current_row.isnull().all()]\n",
    "    print(f\"Subset {subset_index+1}: {', '.join(nan_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def simple_impute(current_subset):\n",
    "    \"\"\"\n",
    "    Impute missing values using SimpleImputer with mean strategy.\n",
    "\n",
    "    Parameters:\n",
    "        current_subset (pandas.DataFrame): Subset of data with missing values.\n",
    "\n",
    "    This function iterates over each column in the given DataFrame and imputes missing values using\n",
    "    the SimpleImputer class from scikit-learn. The imputer is initialized with the 'mean' strategy,\n",
    "    which replaces missing values with the mean of the non-missing values in the column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a SimpleImputer object with 'mean' strategy\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in current_subset.columns:\n",
    "        # Check if any values in the column are missing\n",
    "        if pd.isna(current_subset[col]).any():\n",
    "            # Fit the imputer to the non-missing values in the column\n",
    "            imp.fit(X_test[[col]])\n",
    "            # Transform the missing values in the column\n",
    "            current_subset[col] = imp.transform(current_subset[[col]])\n",
    "            \n",
    "            # Approximate categorical values to the nearest whole number\n",
    "            if col in categorical_columns:\n",
    "                current_subset[col] = np.round(current_subset[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate normal distribution & cGMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.ConditionalGMM.condGMM import CondGMM\n",
    "\n",
    "def cgmm_impute(gmm, missing_features_indices, current_row, number_of_samples):\n",
    "    \"\"\"\n",
    "    Impute missing values using Conditional GMM, returning parameters of the predictive distribution.\n",
    "    \n",
    "    Parameters:\n",
    "        gmm (GMM): Gaussian Mixture Model.\n",
    "        missing_features_indices (list): Indices of missing features.\n",
    "        current_row (pandas.Series): Current row with missing values.\n",
    "        number_of_samples (int): Number of samples to generate.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing samples, means ('mu'), and covariances ('sigma').\n",
    "    \"\"\"\n",
    "    # Find indices of known features\n",
    "    known_features_indices = [i for i in range(len(current_row)) if i not in missing_features_indices]\n",
    "    \n",
    "    # Extract values of known features for the given row\n",
    "    known_features_values = current_row.iloc[known_features_indices].values\n",
    "    \n",
    "    # Initialize CondGMM\n",
    "    cGMM = CondGMM(gmm.weights_, gmm.means_, gmm.covariances_, known_features_indices)\n",
    "    \n",
    "    # Generate samples using Conditional GMM\n",
    "    generated_samples = cGMM.rvs(known_features_values, size=number_of_samples, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Extract mean and covariance for the conditional distribution\n",
    "    mus = cGMM.conditional_component_means(known_features_values)\n",
    "    c_weights = cGMM.conditional_weights(known_features_values)\n",
    "    c_weights = c_weights[:, np.newaxis]  # Ensure weights are aligned for broadcasting\n",
    "    mu = np.sum(c_weights * mus, axis=0)  # Weighted sum across the correct axis\n",
    "    sigma = cGMM.conditional_component_covs()\n",
    "    \n",
    "    # Replace NaN values with 0, if there are any\n",
    "    if any(np.isnan(x) for x in mu):\n",
    "        mu = np.nan_to_num(mu)\n",
    "\n",
    "    return {\n",
    "        \"samples\": generated_samples,\n",
    "        \"mu\": mu,\n",
    "        \"sigma\": sigma\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAEAC & GAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaeac_gain_impute(model, missing_features_indices, current_row, number_of_samples):\n",
    "    \"\"\"\n",
    "    Impute missing values using a Variational Autoencoder or Generative Adversarial Imputation Network.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Trained Variational Autoencoder or GAIN model.\n",
    "        missing_features_indices (list): Indices of missing features.\n",
    "        current_row (pandas.Series): Current row with missing values.\n",
    "        number_of_samples (int): Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: Generated data with imputed missing values.\n",
    "    \"\"\"\n",
    "    generated_samples = np.empty((number_of_samples, len(missing_features_indices)))\n",
    "    \n",
    "    # Repeat the prediction process for the specified number of samples\n",
    "    for i in range(number_of_samples):\n",
    "        # Impute missing values for each feature index\n",
    "        # Reshape the missing values to a 2D array with one row and all missing features\n",
    "        missing_features_values = model.predict(current_row.values.reshape(1, -1).astype(np.float32), verbose=0)\n",
    "        \n",
    "        # Store the generated data in the array\n",
    "        generated_samples[i] = missing_features_values[:, missing_features_indices]\n",
    "    \n",
    "    return {\n",
    "        \"samples\": generated_samples\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute multiple missing data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_and_clip(value, min_val, max_val):\n",
    "    # Round to the nearest integer\n",
    "    rounded_value = round(value)\n",
    "    # Clip to the range [min_val, max_val]\n",
    "    clipped_value = max(min_val, min(rounded_value, max_val))\n",
    "    return int(clipped_value)\n",
    "\n",
    "def cluster_categorical_features(value, col_index):\n",
    "    col_name = X.columns[col_index]\n",
    "    min_value = X[col_name].min()\n",
    "    max_value = X[col_name].max()\n",
    "    return round_and_clip(value, min_value, max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def imputing_missing_data(subsets, method='simple', number_of_samples=default_number_of_samples, model=None):\n",
    "    \"\"\"\n",
    "    Impute missing data in subsets using different imputation methods.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Imputation method ('simple', 'multivariate', 'cgmm', 'vaeac', or 'gain').\n",
    "        model: Trained model for certain imputation methods.\n",
    "    \"\"\"\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        if method == 'simple':\n",
    "            # Simple Imputer\n",
    "            generated_data = simple_impute(subset)\n",
    "        else:\n",
    "            # Initialize to keep track of actual row index, because indices were shuffled\n",
    "            row_in_subset_index = 0\n",
    "            \n",
    "            for row_index, row in subset.iterrows():\n",
    "                # Get indices of unknown features\n",
    "                missing_features_indices = [row.index.get_loc(col) for col in row.index if pd.isna(row[col])]\n",
    "                \n",
    "                # If all features are known, continue   \n",
    "                if len(missing_features_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                generated_data = None\n",
    "                \n",
    "                if method == 'multivariate' or method == 'cgmm':\n",
    "                    # Multivariate Imputer or Conditional GMM\n",
    "                    generated_data = cgmm_impute(model, missing_features_indices, row, number_of_samples)\n",
    "                elif method == 'vaeac' or method == 'gain':\n",
    "                    # Variational AutoEncoder or Generative Adversarial Imputation Network\n",
    "                    generated_data = vaeac_gain_impute(model, missing_features_indices, row, number_of_samples)\n",
    "                    \n",
    "                # Update unknown features with sampled data\n",
    "                for feature_index in range(len(missing_features_indices)):\n",
    "                    # Check if generated_data is a dictionary\n",
    "                    if isinstance(generated_data, dict):\n",
    "                        if 'mu' in generated_data and 'sigma' in generated_data:\n",
    "                            # Convert mu and sigma to lists if they are numpy arrays\n",
    "                            mu = generated_data['mu'].tolist() if isinstance(generated_data['mu'], np.ndarray) else generated_data['mu']\n",
    "                            sigma = generated_data['sigma'].tolist() if isinstance(generated_data['sigma'], np.ndarray) else generated_data['sigma']\n",
    "                            samples = [sample[feature_index] for sample in generated_data['samples']]\n",
    "                            \n",
    "                            # Apply rounding and clipping to categorical features\n",
    "                            if X.columns[missing_features_indices[feature_index]] in categorical_columns:\n",
    "                                samples = [cluster_categorical_features(sample, missing_features_indices[feature_index]) for sample in samples]\n",
    "                            \n",
    "                            data_to_insert = json.dumps({\n",
    "                                \"samples\": samples,\n",
    "                                \"mu\": mu,\n",
    "                                \"sigma\": sigma\n",
    "                            })\n",
    "                        else:\n",
    "                            samples = [sample[feature_index] for sample in generated_data['samples']]\n",
    "                            data_to_insert = json.dumps({\n",
    "                                \"samples\": samples\n",
    "                            })\n",
    "                        \n",
    "                        subset.at[row_index, subset.columns[missing_features_indices[feature_index]]] = data_to_insert\n",
    "                \n",
    "                row_in_subset_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from properscoring import crps_ensemble\n",
    "\n",
    "def get_scoring(subsets, method='simple', print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate scores (NMSE, Log Score, and CRPS) for features in subsets of data.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Imputation method ('simple', 'multivariate', 'cgmm', 'vaeac', or 'gain').\n",
    "        print_results (bool): Whether to print scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing scores for each feature in the subsets organized by score type.\n",
    "    \"\"\"\n",
    "    # Convert method to lowercase for case-insensitive comparison\n",
    "    method = method.lower()\n",
    "    \n",
    "    # Deserialize any strings in subsets\n",
    "    for subset in subsets:\n",
    "        for col in subset.columns:\n",
    "            subset[col] = subset[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    all_subsets_scores = {}  # Dictionary to store scores for each subset\n",
    "    \n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        feature_scores = {}  # Dictionary for each type of score per feature\n",
    "        \n",
    "        # Identify features with missing values\n",
    "        if method == 'simple':\n",
    "            missing_features_indices = [col_index for col_index, col in enumerate(list_of_subsets[subset_index].columns) if list_of_subsets[subset_index][col].isnull().all()]\n",
    "        else:\n",
    "            missing_features_indices = [col_index for col_index, col in enumerate(subset.columns) if subset[col].apply(lambda x: isinstance(x, (list, dict))).any()]\n",
    "\n",
    "        if not missing_features_indices:\n",
    "            continue  # Skip if no missing values\n",
    "        \n",
    "        # Calculate scores for each row in the subset\n",
    "        for row_index, row in subset.iterrows():\n",
    "            try:\n",
    "                original_values = X.iloc[row_index, missing_features_indices].values\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "            # Calculate scores for each feature with missing values\n",
    "            for feature_index, col_index in enumerate(missing_features_indices):\n",
    "                feature_name = subset.columns[col_index]\n",
    "                generated_samples = row.iloc[col_index]\n",
    "                original_value = original_values[feature_index]\n",
    "                \n",
    "                # Initialize dictionary only with NMSE, others will be added as needed\n",
    "                if feature_name not in feature_scores:\n",
    "                    feature_scores[feature_name] = {'nmse': []}\n",
    "                \n",
    "                # If generated samples is not a dictionary, wrap it in a list\n",
    "                if not isinstance(generated_samples, dict):\n",
    "                    generated_samples = {'samples': [generated_samples]}\n",
    "\n",
    "                # Calculate Log Score\n",
    "                if 'mu' in generated_samples and 'sigma' in generated_samples:\n",
    "                    epsilon = 1e-10\n",
    "                    \n",
    "                    # Read mean and covariance\n",
    "                    mu = np.mean(generated_samples['mu']) if isinstance(generated_samples['mu'], (list, np.ndarray)) else generated_samples['mu']\n",
    "                    sigma = np.mean(generated_samples['sigma']) if isinstance(generated_samples['sigma'], (list, np.ndarray)) else generated_samples['sigma']\n",
    "                    sigma = max(sigma, epsilon)  # Ensure sigma is positive\n",
    "                    \n",
    "                    # Check for NaN values\n",
    "                    if np.isnan(mu) or np.isnan(sigma):\n",
    "                        continue\n",
    "                    \n",
    "                    # Log score calculation\n",
    "                    log_score = -np.log(norm.pdf(original_value, loc=mu, scale=sigma) + epsilon)\n",
    "                    \n",
    "                    feature_scores[feature_name].setdefault('log_score', []).append(log_score)\n",
    "                \n",
    "                # Calculate CRPS for ensemble predictions\n",
    "                elif 'samples' in generated_samples:\n",
    "                    samples_size = len(generated_samples['samples'])\n",
    "                    original_value_array = np.repeat(original_value, samples_size)\n",
    "                    crps_score = crps_ensemble(original_value_array, generated_samples['samples'])\n",
    "                    feature_scores[feature_name].setdefault('crps', []).append(crps_score)\n",
    "                \n",
    "                # Calculate Mean Squared Error\n",
    "                squared_errors = [(original_value - x)**2 for x in generated_samples['samples']]\n",
    "                feature_scores[feature_name]['nmse'].append(squared_errors)\n",
    "        \n",
    "        # Average scores for each feature\n",
    "        for feature_name, scores in feature_scores.items():\n",
    "            for score_type, values in scores.items():\n",
    "                if values:\n",
    "                    # Check for NaN values\n",
    "                    if np.isnan(values).any():\n",
    "                        raise Exception(\"NaN values found in scores!\")\n",
    "                    \n",
    "                    # Calculate mean score\n",
    "                    mean_score = np.mean(values)\n",
    "                    \n",
    "                    if score_type == 'nmse':\n",
    "                        variance = np.var(X_test[feature_name])\n",
    "                        \n",
    "                        # Confirm that variance is not 0, handle it\n",
    "                        if variance == 0:\n",
    "                            if mean_score != 0:\n",
    "                                raise Exception(\"Mean Squared Error cannot be different than 0 when variance is equal to 0!\")\n",
    "                            else:\n",
    "                                mean_score = 0\n",
    "                        else:\n",
    "                            mean_score = mean_score / variance\n",
    "                            \n",
    "                    # Round the mean score to 3 decimal places\n",
    "                    feature_scores[feature_name][score_type] = np.round(mean_score, 3)\n",
    "        \n",
    "        all_subsets_scores[subset_index] = feature_scores\n",
    "        \n",
    "        # Print scores if required\n",
    "        if print_results:\n",
    "            print(f\"Scores for Subset {subset_index + 1}:\")\n",
    "            for feature_name, scores in feature_scores.items():\n",
    "                print(f\"Feature {feature_name}: \", end=\"\")\n",
    "                for score_type, score_value in scores.items():\n",
    "                    print(f\"{score_type.upper()} = {score_value}, \", end=\"\")\n",
    "                print()  # New line for each feature\n",
    "            \n",
    "    return all_subsets_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message=\"X does not have valid feature names\")\n",
    "\n",
    "# Load the trained classifier model\n",
    "classifier = load('..\\helpers\\predictive_models\\cardio_classifier.h5')\n",
    "\n",
    "def get_classification_result(subsets, method='simple', should_print=False):\n",
    "    \"\"\"\n",
    "    Calculate accuracy and AUC scores for subsets of data using a trained classifier.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Imputation method ('simple', 'multivariate', 'cgmm', 'vaeac', or 'gain').\n",
    "        should_print (bool): Whether to print AUC scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of accuracy and AUC scores for each subset.\n",
    "    \"\"\"\n",
    "    # Convert method to lowercase for case-insensitive comparison\n",
    "    method = method.lower()\n",
    "    \n",
    "    # Deserialize any strings in subsets\n",
    "    for subset in subsets:\n",
    "        for col_index in subset.columns:\n",
    "            subset[col_index] = subset[col_index].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Initialize lists to store classification results and scores\n",
    "    classification_results = []\n",
    "    accuracy_per_subset = []\n",
    "    auc_per_subset = []\n",
    "\n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        subset_results = []\n",
    "        \n",
    "        # Iterate through rows in the subset DataFrame\n",
    "        for row_index, row in subset.iterrows():\n",
    "            output_probs = []\n",
    "            \n",
    "            # Process each row based on the method used\n",
    "            if method != 'simple':\n",
    "                serialized_arrays = []\n",
    "                non_serialized_values = []\n",
    "                \n",
    "                # Split row values into serialized arrays and non-serialized values\n",
    "                for col_index, value in enumerate(row):\n",
    "                    if isinstance(value, dict):\n",
    "                        serialized_arrays.append((col_index, value['samples']))\n",
    "                    else:\n",
    "                        non_serialized_values.append((col_index, value))\n",
    "                \n",
    "                # Generate combined rows by combining serialized arrays with non-serialized values\n",
    "                for i in range(default_number_of_samples):\n",
    "                    combined_row = non_serialized_values.copy()\n",
    "                    \n",
    "                    # For each serialized array, append the corresponding value at the current index\n",
    "                    for col_index, serialized_array in serialized_arrays:\n",
    "                        assert len(serialized_array) == default_number_of_samples\n",
    "                        combined_row.append((col_index, serialized_array[i]))\n",
    "                    \n",
    "                    # Set the combined row in a correct order of features\n",
    "                    combined_row_array = np.zeros(shape=len(combined_row))\n",
    "                    for col_index, value in combined_row:\n",
    "                        combined_row_array[col_index] = value\n",
    "\n",
    "                    \n",
    "                    output_probs.append(combined_row_array)\n",
    "            else:\n",
    "                output_probs.append(row.values.tolist())\n",
    "            \n",
    "            # Predict probabilities\n",
    "            predicted_probs = classifier.predict_proba(np.vstack(output_probs))\n",
    "            subset_results.append(predicted_probs)\n",
    "        \n",
    "        classification_results.append(subset_results)\n",
    "\n",
    "    # Create an empty list to store dictionaries of results\n",
    "    results_list = []\n",
    "\n",
    "    # Calculate AUC scores and accuracy for each subset\n",
    "    for subset_index, subset_results in enumerate(classification_results):\n",
    "        true_labels = y.loc[subsets[subset_index].index]\n",
    "        \n",
    "        subset_predicted_probs = []\n",
    "        \n",
    "        # Determine predicted probabilities for each row in the subset\n",
    "        for output_probs in subset_results:\n",
    "            predicted_prob = np.mean(output_probs[:, 1])\n",
    "            subset_predicted_probs.append(predicted_prob)\n",
    "        \n",
    "        # Convert probabilities to binary predictions based on the threshold\n",
    "        subset_predictions = [1 if prob > 0.5 else 0 for prob in subset_predicted_probs]\n",
    "        \n",
    "        subset_accuracy = accuracy_score(true_labels, subset_predictions)\n",
    "        subset_auc = roc_auc_score(true_labels, subset_predicted_probs)\n",
    "        \n",
    "        accuracy_per_subset.append(round(subset_accuracy, 2))\n",
    "        auc_per_subset.append(round(subset_auc, 2))\n",
    "        \n",
    "        # Append results to the list of dictionaries\n",
    "        results_list.append({'Accuracy': np.round(subset_accuracy * 100, 2), 'AUC': np.round(subset_auc, 3)})\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    results_table = pd.DataFrame(results_list)\n",
    "\n",
    "    # Print classification scores in a table\n",
    "    if should_print:\n",
    "        print(results_table)\n",
    "        \n",
    "    return accuracy_per_subset, auc_per_subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleImputer with mean strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(imputer_subsets, 'simple')\n",
    "\n",
    "imputer_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer_score = get_scoring(imputer_subsets, 'simple', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer_accuracy, simple_imputer_auc = get_classification_result(imputer_subsets, 'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['simple_imputer'] = {'score': simple_imputer_score, 'accuracy': simple_imputer_accuracy, 'auc': simple_imputer_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create Gaussian Mixture Model with a single component\n",
    "gmm = GaussianMixture(n_components=1, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(multivariate_subsets, 'multivariate', default_number_of_samples, gmm)\n",
    "\n",
    "multivariate_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_score = get_scoring(multivariate_subsets, 'multivariate', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_accuracy, multivariate_auc = get_classification_result(multivariate_subsets, 'multivariate', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['multivariate'] = {'score': multivariate_score, 'accuracy': multivariate_accuracy, 'auc': multivariate_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_bic(data, n_components_range):\n",
    "    \"\"\"\n",
    "    Computes the Bayesian Information Criterion (BIC) for Gaussian Mixture Models\n",
    "    with different numbers of components.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): Input data.\n",
    "        n_components_range (range): Range of number of components to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        list: BIC values for each number of components.\n",
    "    \"\"\"\n",
    "    # List to store BIC values\n",
    "    bic = []\n",
    "    \n",
    "    # Loop through number of components and compute BIC for each\n",
    "    for n_components in n_components_range:\n",
    "        # Create Gaussian Mixture Model with specified number of components\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        gmm.fit(data)  # Fit the model to the data\n",
    "        bic.append(gmm.bic(data))  # Calculate BIC and add to list\n",
    "    \n",
    "    return bic  # Return list of BIC values\n",
    "\n",
    "# Used to simplify getting optimal number of components based on previous run\n",
    "optimal_n_components = None\n",
    "\n",
    "if optimal_n_components is None:\n",
    "    # Range of number of components to evaluate\n",
    "    n_components_range = range(1, 51)\n",
    "\n",
    "    # Compute BIC values\n",
    "    bic_values = compute_bic(X_train, n_components_range)\n",
    "\n",
    "    # Optimal number of components\n",
    "    optimal_n_components = n_components_range[np.argmin(bic_values)]\n",
    "\n",
    "    # Plotting BIC values\n",
    "    plt.plot(n_components_range, bic_values, marker='o', label='BIC Values')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('BIC Value')\n",
    "    plt.title('BIC for Gaussian Mixture Models')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    plt.savefig('../images/without_missingness/bic.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Mixture Model with optimal number of components\n",
    "gmm = GaussianMixture(n_components=optimal_n_components, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(cgmm_subsets, 'cgmm', default_number_of_samples, gmm)\n",
    "\n",
    "cgmm_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_score = get_scoring(cgmm_subsets, 'cgmm', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_accuracy, cgmm_auc = get_classification_result(cgmm_subsets, 'cgmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['cgmm'] = {'score': cgmm_score, 'accuracy': cgmm_accuracy, 'auc': cgmm_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder with Arbitrary Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaeac_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Perform Singular Value Decomposition (SVD) on training data\n",
    "svd = TruncatedSVD(n_components=min(X_train.shape), random_state=RANDOM_STATE)\n",
    "svd.fit(X_train)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Choose threshold to preserve 90% of total variance\n",
    "threshold_index = np.argmax(cumulative_variance_ratio >= 0.90)\n",
    "threshold = svd.singular_values_[threshold_index]\n",
    "\n",
    "print(f\"Starting threshold to preserve 90% of total variance: {threshold}\")\n",
    "\n",
    "# Analyze singular values\n",
    "singular_values = svd.singular_values_\n",
    "num_non_trivial = np.sum(singular_values > threshold)  # Choose a threshold to determine non-trivial singular values\n",
    "\n",
    "# Select latent space dimensionality\n",
    "latent_dim = num_non_trivial\n",
    "\n",
    "print(f\"Number of non-trivial singular values: {num_non_trivial}\")\n",
    "print(f\"Selected latent space dimensionality: {latent_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "file_path = \"../images/without_missingness/cumulative_explained_variance.png\"\n",
    "\n",
    "if not os.path.isfile(file_path):\n",
    "    # Plot cumulative explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.arange(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='-')\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.axhline(y=0.9, color='r', linestyle='--', label='90% Explained Variance')\n",
    "    plt.axvline(x=threshold_index + 1, color='g', linestyle='--', label=f'Threshold Component ({threshold_index + 1})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('../images/without_missingness/cumulative_explained_variance.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "inputs = Input(shape=(input_dim,))\n",
    "encoded = inputs\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "z_mean = Dense(latent_dim)(encoded)\n",
    "z_log_var = Dense(latent_dim)(encoded)\n",
    "\n",
    "# Reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Define the decoder\n",
    "decoded = z\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "outputs = Dense(input_dim)(decoded)\n",
    "\n",
    "# Create the VAE model\n",
    "vaeac = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "vaeac.compile(optimizer='adam', loss='mse')  # Use MSE as the reconstruction loss\n",
    "\n",
    "# Train the model\n",
    "history = vaeac.fit(X_train, X_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(vaeac_subsets, 'vaeac', default_number_of_samples, vaeac)\n",
    "\n",
    "vaeac_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaeac_score = get_scoring(vaeac_subsets, 'vaeac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaeac_accuracy, vaeac_auc = get_classification_result(vaeac_subsets, 'vaeac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['vaeac'] = {'score': vaeac_score, 'accuracy': vaeac_accuracy, 'auc': vaeac_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Imputation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = load('..\\helpers\\generative_models\\cardio_gain_generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(gain_subsets, 'gain', default_number_of_samples, gain)\n",
    "\n",
    "gain_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_score = get_scoring(gain_subsets, 'gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_accuracy, gain_auc = get_classification_result(gain_subsets, 'gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['gain'] = {'score': gain_score, 'accuracy': gain_accuracy, 'auc': gain_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Define the directory where results are stored\n",
    "results_directory = '..\\\\results\\\\without_missingness'\n",
    "\n",
    "# Get the list of existing result files to determine the next run number\n",
    "existing_files = os.listdir(results_directory)\n",
    "run_numbers = [int(file.split(\"_\")[1].split(\".\")[0]) for file in existing_files if file.startswith(\"run_\")]\n",
    "\n",
    "# Determine the next run number\n",
    "next_run_number = max(run_numbers, default=0) + 1\n",
    "\n",
    "# Create tables for accuracy, AUC, and scores\n",
    "accuracy_table = [[\"\"] + list(results_dict.keys())]\n",
    "auc_table = [[\"\"] + list(results_dict.keys())]\n",
    "score_table = [[\"\"] + list(results_dict.keys())]\n",
    "\n",
    "for i in range(len(next(iter(results_dict.values()))[\"accuracy\"])):\n",
    "    accuracy_row = [i+1] + [results_dict[key][\"accuracy\"][i] for key in results_dict.keys()]\n",
    "    auc_row = [i+1] + [results_dict[key][\"auc\"][i] for key in results_dict.keys()]\n",
    "    score_row = [i+1] + [results_dict[key][\"score\"].get(i, \"\") for key in results_dict.keys()]\n",
    "    accuracy_table.append(accuracy_row)\n",
    "    auc_table.append(auc_row)\n",
    "    score_table.append(score_row)\n",
    "\n",
    "# Generate the tabulated strings for accuracy, AUC, and scores\n",
    "tabulated_accuracy_table = tabulate(accuracy_table, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "tabulated_auc_table = tabulate(auc_table, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "tabulated_score_table = tabulate(score_table, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "\n",
    "# Define the file name for the new result\n",
    "new_file_name = f\"run_{next_run_number}.txt\"\n",
    "file_path = os.path.join(results_directory, new_file_name)\n",
    "\n",
    "# Save accuracy, AUC, and score tables to the same file with separation\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"Number of datapoints: \" + str(np.round(X_train.shape[0] * fraction_of_data)) + \"\\n\" + \"Number of samples: \" + str(default_number_of_samples) + \"\\n\\n\")\n",
    "    file.write(\"Accuracy:\\n\")\n",
    "    file.write(tabulated_accuracy_table + \"\\n\\n\")\n",
    "    file.write(\"AUC:\\n\")\n",
    "    file.write(tabulated_auc_table + \"\\n\\n\")\n",
    "    file.write(\"Scores:\\n\")\n",
    "    file.write(tabulated_score_table)\n",
    "\n",
    "print(f\"Results saved to {new_file_name}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
