{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_STATE = 404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/cardio_train.csv', delimiter=';')\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['cardio'])\n",
    "y = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# Select columns to be scaled\n",
    "numeric_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
    "categorical_columns = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
    "\n",
    "# Fit and transform your data (only for numeric columns)\n",
    "scaler = StandardScaler()\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "# # Apply one-hot encoding to categorical columns\n",
    "# encoder = OneHotEncoder(sparse_output=False)  # Create the encoder\n",
    "# X_encoded = encoder.fit_transform(X[categorical_columns])  # Fit and transform the categorical data\n",
    "# column_names = encoder.get_feature_names_out(categorical_columns)  # Get new column names for encoded features\n",
    "# X_encoded = pd.DataFrame(X_encoded, columns=column_names)  # Create a DataFrame with the new column names\n",
    "\n",
    "# # Drop original categorical columns and concatenate the new encoded DataFrame\n",
    "# X = X.drop(categorical_columns, axis=1)\n",
    "# X = pd.concat([X, X_encoded], axis=1)\n",
    "    \n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# This script defines the generator and discriminator models for a Generative Adversarial Imputation Network (GAIN)\n",
    "# using the Keras API in TensorFlow 2.x.\n",
    "\n",
    "def build_generator(data_dim, hidden_dim):\n",
    "    \"\"\"\n",
    "    Builds the generator model for a Generative Adversarial Imputation Network (GAIN).\n",
    "\n",
    "    Args:\n",
    "        data_dim (int): The dimensionality of the input data.\n",
    "        hidden_dim (int): The number of hidden units in the encoder and decoder.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The generator model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(data_dim,)),  # Input layer\n",
    "        tf.keras.layers.Dense(hidden_dim, activation='relu'),  # Hidden layers\n",
    "        tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
    "        tf.keras.layers.Dense(data_dim, activation='sigmoid')  # Output layer\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_discriminator(data_dim, hidden_dim):\n",
    "    \"\"\"\n",
    "    Builds the discriminator model for a Generative Adversarial Imputation Network (GAIN).\n",
    "\n",
    "    Args:\n",
    "        data_dim (int): The dimensionality of the input data.\n",
    "        hidden_dim (int): The number of hidden units in the encoder and decoder.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The discriminator model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(data_dim,)),  # Input layer\n",
    "        tf.keras.layers.Dense(hidden_dim, activation='relu'),  # Hidden layers\n",
    "        tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup models\n",
    "data_dim = X_train.shape[1]\n",
    "hidden_dim = 128\n",
    "\n",
    "generator = build_generator(data_dim, hidden_dim)\n",
    "discriminator = build_discriminator(data_dim, hidden_dim)\n",
    "\n",
    "# Print the model summary\n",
    "generator.summary(), discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for the discriminator\n",
    "def discriminator_loss(D_prob, M, X, G_sample):\n",
    "    D_prob = tf.cast(D_prob, dtype=tf.float32)  # Cast to float32\n",
    "    return -tf.reduce_mean(M * tf.math.log(D_prob + 1e-8) + (1 - M) * tf.math.log(1. - D_prob + 1e-8))\n",
    "\n",
    "# Loss function for the generator\n",
    "def generator_loss(D_prob, G_sample, M, X):\n",
    "    # Cast all inputs to float32 to ensure consistent data types for operations\n",
    "    D_prob = tf.cast(D_prob, dtype=tf.float32)\n",
    "    G_sample = tf.cast(G_sample, dtype=tf.float32)\n",
    "    M = tf.cast(M, dtype=tf.float32)\n",
    "    X = tf.cast(X, dtype=tf.float32)\n",
    "    \n",
    "    # Compute the binary cross-entropy loss part\n",
    "    BCE_loss = -tf.reduce_mean((1 - M) * tf.math.log(D_prob + tf.constant(1e-8, dtype=tf.float32)))\n",
    "\n",
    "    # Compute the mean squared error loss part\n",
    "    MSE_loss = tf.reduce_mean(M * tf.square(X - G_sample))\n",
    "\n",
    "    # Weighting factor for the losses\n",
    "    alpha = 0.5\n",
    "\n",
    "    # Combine the losses\n",
    "    total_loss = alpha * BCE_loss + (1 - alpha) * MSE_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "# Adam optimizer is a stochastic gradient descent method\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(generator, discriminator, data, batch_size):\n",
    "    noise = tf.random.normal([batch_size, data.shape[1]])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_data = generator(noise, training=True)\n",
    "        real_output = discriminator(data, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "        gen_loss = generator_loss(fake_output, generated_data, data, noise)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output, data, noise)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "def train_gan(generator, discriminator, df, iterations, batch_size):\n",
    "    for iteration in range(iterations):\n",
    "        idx = np.random.choice(len(df), batch_size, replace=False)\n",
    "        data_batch = df.iloc[idx]\n",
    "        gen_loss, disc_loss = train_step(generator, discriminator, data_batch, batch_size)\n",
    "        if iteration % 1000 == 0:\n",
    "            print(f\"Iteration {iteration}, Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "train_gan(generator, discriminator, X_train, iterations=10000, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(generator, 'cardio_gain_generator.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
