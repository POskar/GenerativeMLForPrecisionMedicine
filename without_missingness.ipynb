{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RANDOM_STATE = 404\n",
    "number_of_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0  18393       2     168    62.0    110     80            1     1      0   \n",
       "1  20228       1     156    85.0    140     90            3     1      0   \n",
       "2  18857       1     165    64.0    130     70            3     1      0   \n",
       "3  17623       2     169    82.0    150    100            1     1      0   \n",
       "4  17474       1     156    56.0    100     60            1     1      0   \n",
       "\n",
       "   alco  active  cardio  \n",
       "0     0       1       0  \n",
       "1     0       1       1  \n",
       "2     0       0       1  \n",
       "3     0       1       1  \n",
       "4     0       0       0  "
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/cardio_train.csv', delimiter=';')\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['cardio'])\n",
    "y = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the results after each method\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.436062</td>\n",
       "      <td>1.364055</td>\n",
       "      <td>0.443452</td>\n",
       "      <td>-0.847873</td>\n",
       "      <td>-0.122182</td>\n",
       "      <td>-0.088238</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.307686</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>-1.018168</td>\n",
       "      <td>0.749831</td>\n",
       "      <td>0.072610</td>\n",
       "      <td>-0.035180</td>\n",
       "      <td>2.400793</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.247997</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>0.078047</td>\n",
       "      <td>-0.708942</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>2.400793</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.748152</td>\n",
       "      <td>1.364055</td>\n",
       "      <td>0.565254</td>\n",
       "      <td>0.541435</td>\n",
       "      <td>0.137541</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.808543</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>-1.018168</td>\n",
       "      <td>-1.264666</td>\n",
       "      <td>-0.187113</td>\n",
       "      <td>-0.194356</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    gender    height    weight     ap_hi     ap_lo  cholesterol  \\\n",
       "0 -0.436062  1.364055  0.443452 -0.847873 -0.122182 -0.088238    -0.539322   \n",
       "1  0.307686 -0.733108 -1.018168  0.749831  0.072610 -0.035180     2.400793   \n",
       "2 -0.247997 -0.733108  0.078047 -0.708942  0.007679 -0.141297     2.400793   \n",
       "3 -0.748152  1.364055  0.565254  0.541435  0.137541  0.017879    -0.539322   \n",
       "4 -0.808543 -0.733108 -1.018168 -1.264666 -0.187113 -0.194356    -0.539322   \n",
       "\n",
       "   gluc  smoke  alco  active  \n",
       "0     0      0     0       1  \n",
       "1     0      0     0       1  \n",
       "2     0      0     0       0  \n",
       "3     0      0     0       1  \n",
       "4     0      0     0       0  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Select columns to be scaled\n",
    "numeric_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'gender', 'cholesterol']\n",
    "categorical_columns = ['gluc', 'smoke', 'alco', 'active']\n",
    "\n",
    "# Fit and transform your data (only for numeric columns)\n",
    "scaler = StandardScaler()\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "# Apply one-hot encoding to categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "      <td>70000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.272227e-16</td>\n",
       "      <td>-1.644399e-16</td>\n",
       "      <td>1.450116e-15</td>\n",
       "      <td>-2.905105e-16</td>\n",
       "      <td>7.623108e-17</td>\n",
       "      <td>1.745905e-17</td>\n",
       "      <td>1.381498e-16</td>\n",
       "      <td>0.226457</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>0.053771</td>\n",
       "      <td>0.803729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>1.000007e+00</td>\n",
       "      <td>0.572270</td>\n",
       "      <td>0.283484</td>\n",
       "      <td>0.225568</td>\n",
       "      <td>0.397179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.514407e+00</td>\n",
       "      <td>-7.331083e-01</td>\n",
       "      <td>-1.332014e+01</td>\n",
       "      <td>-4.460075e+00</td>\n",
       "      <td>-1.810381e+00</td>\n",
       "      <td>-8.841161e-01</td>\n",
       "      <td>-5.393221e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.315341e-01</td>\n",
       "      <td>-7.331083e-01</td>\n",
       "      <td>-6.527630e-01</td>\n",
       "      <td>-6.394770e-01</td>\n",
       "      <td>-5.725127e-02</td>\n",
       "      <td>-8.823850e-02</td>\n",
       "      <td>-5.393221e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.489744e-02</td>\n",
       "      <td>-7.331083e-01</td>\n",
       "      <td>7.804703e-02</td>\n",
       "      <td>-1.532192e-01</td>\n",
       "      <td>-5.725127e-02</td>\n",
       "      <td>-8.823850e-02</td>\n",
       "      <td>-5.393221e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.531244e-01</td>\n",
       "      <td>1.364055e+00</td>\n",
       "      <td>6.870554e-01</td>\n",
       "      <td>5.414349e-01</td>\n",
       "      <td>7.261016e-02</td>\n",
       "      <td>-3.517999e-02</td>\n",
       "      <td>9.307354e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.720199e+00</td>\n",
       "      <td>1.364055e+00</td>\n",
       "      <td>1.043119e+01</td>\n",
       "      <td>8.738353e+00</td>\n",
       "      <td>1.031826e+02</td>\n",
       "      <td>5.785165e+01</td>\n",
       "      <td>2.400793e+00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        gender        height        weight         ap_hi  \\\n",
       "count  7.000000e+04  7.000000e+04  7.000000e+04  7.000000e+04  7.000000e+04   \n",
       "mean   5.272227e-16 -1.644399e-16  1.450116e-15 -2.905105e-16  7.623108e-17   \n",
       "std    1.000007e+00  1.000007e+00  1.000007e+00  1.000007e+00  1.000007e+00   \n",
       "min   -3.514407e+00 -7.331083e-01 -1.332014e+01 -4.460075e+00 -1.810381e+00   \n",
       "25%   -7.315341e-01 -7.331083e-01 -6.527630e-01 -6.394770e-01 -5.725127e-02   \n",
       "50%    9.489744e-02 -7.331083e-01  7.804703e-02 -1.532192e-01 -5.725127e-02   \n",
       "75%    7.531244e-01  1.364055e+00  6.870554e-01  5.414349e-01  7.261016e-02   \n",
       "max    1.720199e+00  1.364055e+00  1.043119e+01  8.738353e+00  1.031826e+02   \n",
       "\n",
       "              ap_lo   cholesterol          gluc         smoke          alco  \\\n",
       "count  7.000000e+04  7.000000e+04  70000.000000  70000.000000  70000.000000   \n",
       "mean   1.745905e-17  1.381498e-16      0.226457      0.088129      0.053771   \n",
       "std    1.000007e+00  1.000007e+00      0.572270      0.283484      0.225568   \n",
       "min   -8.841161e-01 -5.393221e-01      0.000000      0.000000      0.000000   \n",
       "25%   -8.823850e-02 -5.393221e-01      0.000000      0.000000      0.000000   \n",
       "50%   -8.823850e-02 -5.393221e-01      0.000000      0.000000      0.000000   \n",
       "75%   -3.517999e-02  9.307354e-01      0.000000      0.000000      0.000000   \n",
       "max    5.785165e+01  2.400793e+00      2.000000      1.000000      1.000000   \n",
       "\n",
       "             active  \n",
       "count  70000.000000  \n",
       "mean       0.803729  \n",
       "std        0.397179  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52500, 11), (17500, 11))"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing 10 subsets with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets with 52 datapoints and their columns with missing values:\n",
      "Subset 1: age\n",
      "Subset 2: cholesterol\n",
      "Subset 3: height, alco\n",
      "Subset 4: age, smoke\n",
      "Subset 5: height, ap_hi, alco\n",
      "Subset 6: age, height, smoke\n",
      "Subset 7: height, ap_hi, ap_lo, alco\n",
      "Subset 8: gender, height, weight, smoke\n",
      "Subset 9: age, ap_hi, ap_lo, gluc, smoke\n",
      "Subset 10: age, gender, height, weight, gluc, smoke\n"
     ]
    }
   ],
   "source": [
    "# Function that randomly removes features and replace their values with NaN\n",
    "def remove_features(num_features_to_remove=None, feature_indices_to_remove=None):\n",
    "    \"\"\"\n",
    "    Randomly removes features from a subset of data and replaces their values with NaN.\n",
    "    \n",
    "    Parameters:\n",
    "        num_features_to_remove (int): Number of features to remove randomly.\n",
    "        feature_indices_to_remove (array-like): Indices of features to remove.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Subset of data with removed features and NaN values.\n",
    "    \"\"\"\n",
    "    # Sample a subset of data\n",
    "    subset = X_train.sample(frac=0.001, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Determine features to remove based on number or indices provided\n",
    "    if feature_indices_to_remove is None:\n",
    "        if num_features_to_remove is None:\n",
    "            num_features_to_remove = np.random.randint(1, min(5, len(X_train.columns) - 1))\n",
    "        features_to_remove = np.random.choice(subset.columns[:-1], num_features_to_remove, replace=False)\n",
    "    else:\n",
    "        features_to_remove = subset.columns[feature_indices_to_remove]\n",
    "    \n",
    "    # Replace values of selected features with NaN\n",
    "    features_to_remove = np.random.choice(subset.columns[:-1], num_features_to_remove, replace=False)\n",
    "    subset = subset.astype(object)\n",
    "    subset.loc[:, features_to_remove] = np.nan\n",
    "    \n",
    "    return subset\n",
    "\n",
    "list_of_subsets = []\n",
    "subset_without_changes = X_train.sample(frac=0.001, random_state=RANDOM_STATE)\n",
    "\n",
    "# Generate subsets with varying numbers of removed features\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(1))\n",
    "\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(2))\n",
    "\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(3))\n",
    "\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(4))\n",
    "\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(np.random.randint(5, 7)))\n",
    "\n",
    "# Print information about subsets and their missing columns\n",
    "print(f'Subsets with {list_of_subsets[0].shape[0]} datapoints and their columns with missing values:')\n",
    "for subset_index, subset in enumerate(list_of_subsets):\n",
    "    nan_columns = subset.columns[subset.isnull().all()]\n",
    "    print(f\"Subset {subset_index+1}: {', '.join(nan_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from ConditionalGMM.condGMM import CondGMM\n",
    "import json\n",
    "\n",
    "def imputing_missing_data(subsets, method='simple', model=None):\n",
    "    for subset in subsets:\n",
    "        if method == 'simple':\n",
    "            # Simple Imputer\n",
    "            generated_data = simple_impute(subset)\n",
    "            continue\n",
    "        \n",
    "        # Initialize to keep track of actual row index, because indices were shuffled\n",
    "        row_in_subset_index = 0\n",
    "        \n",
    "        for row_index, row in subset.iterrows():\n",
    "            # Get indices and values of unknown and known features\n",
    "            missing_features_indices = [row.index.get_loc(col) for col in row.index if pd.isna(row[col])]\n",
    "            \n",
    "            # If all features are known, continue\n",
    "            if len(missing_features_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            generated_data = None\n",
    "            \n",
    "            if method == 'multivariate' or method == 'cgmm':\n",
    "                # Multivariate Imputer or Conditional GMM\n",
    "                generated_data = cgmm_impute(model, missing_features_indices, row_in_subset_index)\n",
    "                continue\n",
    "            elif method == 'vae':\n",
    "                # Variational AutoEncoder\n",
    "                generated_data = vae_impute(model, missing_features_indices, row)\n",
    "            \n",
    "            # Update unknown features with sampled data\n",
    "            for feature_index in range(len(missing_features_indices)):\n",
    "                if subset.columns[missing_features_indices[feature_index]] in categorical_columns:\n",
    "                    # Approximate categorical values to the nearest whole number\n",
    "                    generated_data[:, feature_index] = np.round(generated_data[:, feature_index])\n",
    "                subset.iloc[row_in_subset_index, missing_features_indices[feature_index]] = json.dumps([generated_data[sample_index][feature_index] for sample_index in range(generated_data.shape[0])])\n",
    "            \n",
    "            row_in_subset_index += 1\n",
    "\n",
    "def simple_impute(current_subset):\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    for col in current_subset.columns:\n",
    "        if pd.isna(current_subset[col]).any():\n",
    "            imp.fit(X_train[[col]])\n",
    "            current_subset[col] = imp.transform(current_subset[[col]])\n",
    "\n",
    "def cgmm_impute(gmm, missing_features_indices, row_in_subset_index):\n",
    "    # Find indices of known features\n",
    "    known_features_indexes = list(set(range(subset.shape[1])) - set(missing_features_indices))\n",
    "    \n",
    "    # Extract values of known features for the given row\n",
    "    known_features_values = subset.iloc[row_in_subset_index, known_features_indexes]\n",
    "    \n",
    "    # Initialize CondGMM\n",
    "    cGMM = CondGMM(gmm.weights_, gmm.means_, gmm.covariances_, known_features_indexes)\n",
    "    \n",
    "    # Generate samples using Conditional GMM\n",
    "    generated_data = cGMM.rvs(known_features_values, size=number_of_samples, random_state=RANDOM_STATE)\n",
    "    \n",
    "    return generated_data\n",
    "\n",
    "def vae_impute(model, missing_features_indices, current_row):\n",
    "    generated_data = np.empty((number_of_samples, len(missing_features_indices)))\n",
    "        \n",
    "    # Repeat the prediction process for the specified number of samples\n",
    "    for _ in range(number_of_samples):\n",
    "        imputed_values_row = []\n",
    "        # Impute missing values for each feature index\n",
    "        for feature_index in missing_features_indices:\n",
    "            # Impute missing value using the VAE for the current feature and row\n",
    "            imputed_value = model.predict(current_row.values.reshape(1, -1).astype(np.float32), verbose=0)[0, feature_index]\n",
    "            imputed_values_row.append(imputed_value)\n",
    "        generated_data[_] = imputed_values_row\n",
    "        \n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_scoring(subsets, method, should_print=False):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) scores for features in subsets of data.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Method used for imputation ('simple', 'multivariate', 'cgmm', or 'vae').\n",
    "        should_print (bool): Whether to print MSE scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing MSE scores for each feature in the subsets.\n",
    "    \"\"\"    \n",
    "    method = method.lower()\n",
    "    if method == 'multivariate' or method == 'cgmm':\n",
    "        # Convert serialized arrays in each subset to lists\n",
    "        for subset in subsets:\n",
    "            for col in subset.columns:\n",
    "                subset[col] = subset[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        feature_score = {}  # Initialize dictionary to store MSE values\n",
    "        \n",
    "        # Determine unknown features indexes dynamically for each subset\n",
    "        if method == 'simple':\n",
    "            missing_features_indices = [col_index for col_index, col in enumerate(list_of_subsets[subset_index].columns) if list_of_subsets[subset_index][col].isnull().all()]\n",
    "        else:\n",
    "            missing_features_indices = [col_index for col_index, col in enumerate(subset.columns) if subset[col].apply(lambda x: isinstance(x, list)).any()]\n",
    "\n",
    "        if not missing_features_indices:\n",
    "            continue  # Skip if there are no missing values\n",
    "        \n",
    "        # Iterate through rows in the subset DataFrame\n",
    "        for index, row in subset.iterrows():\n",
    "            original_values = X.iloc[index, missing_features_indices].values\n",
    "            \n",
    "            # Compute MSE for each feature separately\n",
    "            for feature_index in range(len(missing_features_indices)):\n",
    "                if method == 'simple':\n",
    "                    generated_samples = [row.iloc[missing_features_indices].values[feature_index]]\n",
    "                else:    \n",
    "                    generated_samples_raw = row.iloc[missing_features_indices].values[feature_index]\n",
    "                    generated_samples = [sample for sample in generated_samples_raw if not pd.isna(sample)]\n",
    "                \n",
    "                # Grab the original value of the feature\n",
    "                original_value = original_values[feature_index]\n",
    "                \n",
    "                for sample in generated_samples:\n",
    "                    squared_error = (original_value - sample)**2\n",
    "                    \n",
    "                    if missing_features_indices[feature_index] not in feature_score:\n",
    "                        feature_score[missing_features_indices[feature_index]] = []\n",
    "                        \n",
    "                    feature_score[missing_features_indices[feature_index]].append(squared_error)\n",
    "        \n",
    "        for feature_index, score_values in feature_score.items():\n",
    "            mse = np.mean(score_values)\n",
    "            # Not sure if it should be X, X_train, or subset_without_changes\n",
    "            variance = np.var(subset_without_changes.iloc[:, feature_index])\n",
    "            nmse = mse / variance\n",
    "            feature_score[feature_index] = nmse\n",
    "        \n",
    "        # Print MSE scores if required\n",
    "        if should_print:\n",
    "            print(f\"MSE for Subset {subset_index + 1}:\")\n",
    "            for feature_index, score_values in feature_score.items():\n",
    "                print(f\"Feature {subset_without_changes.columns[feature_index]}: MSE = {score_values}\")\n",
    "    \n",
    "    # Return dictionary containing MSE scores\n",
    "    return feature_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message=\"X does not have valid feature names\")\n",
    "\n",
    "# Load the trained classifier model\n",
    "classifier = load('classifiers\\cardio_classifier.h5')\n",
    "\n",
    "def get_accuracy(subsets, method, should_print=False):\n",
    "    \"\"\"\n",
    "    Calculate accuracy scores for subsets of data using a trained classifier.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Method used for imputation ('simple', 'multivariate', or 'cgmm').\n",
    "        should_print (bool): Whether to print accuracy scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of accuracy scores for each subset.\n",
    "    \"\"\"\n",
    "    method = method.lower()\n",
    "    \n",
    "    if method == 'multivariate' or method == 'cgmm':\n",
    "        # Convert serialized arrays in each subset to lists\n",
    "        for subset in subsets:\n",
    "            for col in subset.columns:\n",
    "                subset[col] = subset[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    classification_results = []  # Initialize list to store classification results\n",
    "    accuracy_per_subset = []  # Initialize list to store accuracy scores\n",
    "\n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        subset_results = []  # Initialize list to store results for the current subset\n",
    "        \n",
    "        # Iterate through rows in the subset DataFrame\n",
    "        for row_index, row in subset.iterrows():\n",
    "            row_results = []  # Initialize list to store results for the current row\n",
    "            \n",
    "            # Process each row based on the method used\n",
    "            if method != 'simple':\n",
    "                serialized_arrays = []\n",
    "                non_serialized_values = []\n",
    "                \n",
    "                # Split row values into serialized arrays and non-serialized values\n",
    "                for col, value in row.items():\n",
    "                    if isinstance(value, list):\n",
    "                        serialized_arrays.append((col, value))\n",
    "                    else:\n",
    "                        non_serialized_values.append((col, value))\n",
    "                \n",
    "                # Generate combined rows by combining serialized arrays with non-serialized values\n",
    "                for i in range(number_of_samples):\n",
    "                    combined_row = non_serialized_values.copy()\n",
    "                    \n",
    "                    for col, serialized_array in serialized_arrays:\n",
    "                        if i < len(serialized_array):\n",
    "                            combined_row.append((col, serialized_array[i]))\n",
    "                    \n",
    "                    combined_row_array = [value for _, value in combined_row]\n",
    "                    \n",
    "                    try:\n",
    "                        result_array = classifier.predict([combined_row_array], verbose=0)\n",
    "                        row_results.append(result_array)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing row {row_index}: {e}\")\n",
    "            else:\n",
    "                # For simple method, predict directly from row values\n",
    "                result_array = classifier.predict([row.values.tolist()], verbose=0)\n",
    "                row_results.append(result_array)\n",
    "\n",
    "            subset_results.append(row_results)  # Append results for the current row to the subset results\n",
    "        \n",
    "        classification_results.append(subset_results)  # Append subset results to the classification results\n",
    "\n",
    "    # Calculate accuracy scores for each subset\n",
    "    for subset_index, subset_results in enumerate(classification_results):\n",
    "        true_labels = y.loc[subsets[subset_index].index]  # Get true labels for the current subset\n",
    "        \n",
    "        subset_predicted_labels = []  # Initialize list to store predicted labels for the subset\n",
    "        \n",
    "        # Determine predicted labels for each row in the subset\n",
    "        for row_results in subset_results:\n",
    "            predicted_label = 1 if row_results[0] > 0.5 else 0  # Assuming threshold of 0.5\n",
    "            subset_predicted_labels.append(predicted_label)\n",
    "        \n",
    "        subset_accuracy = accuracy_score(true_labels, subset_predicted_labels)  # Calculate accuracy score for the subset\n",
    "        \n",
    "        accuracy_per_subset.append(subset_accuracy)  # Append accuracy score to the list\n",
    "\n",
    "    # Print accuracy scores if required\n",
    "    if (should_print):\n",
    "        for subset_index, subset_accuracy in enumerate(accuracy_per_subset):\n",
    "            print(\"Subset\", subset_index+1, \"accuracy:\", subset_accuracy)\n",
    "        \n",
    "    return accuracy_per_subset  # Return list of accuracy scores for each subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleImputer with mean strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "imputer_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(imputer_subsets, 'simple')\n",
    "\n",
    "imputer_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer_score = get_scoring(imputer_subsets, 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer_accuracy = get_accuracy(imputer_subsets, 'simple', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['simple_imputer'] = {'score': simple_imputer_score, 'accuracy': simple_imputer_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create Gaussian Mixture Model with a single component\n",
    "gmm = GaussianMixture(n_components=1, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(multivariate_subsets, 'multivariate', gmm)\n",
    "\n",
    "multivariate_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_score = get_scoring(multivariate_subsets, 'multivariate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_accuracy = get_accuracy(multivariate_subsets, 'multivariate', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['multivariate'] = {'score': multivariate_score, 'accuracy': multivariate_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BIC to get the optimal number of components for GMM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_bic(data, n_components_range):\n",
    "    \"\"\"\n",
    "    Computes the Bayesian Information Criterion (BIC) for Gaussian Mixture Models with different numbers of components.\n",
    "    \n",
    "    Parameters:\n",
    "        X (array-like): Input data.\n",
    "        n_components_range (range): Range of number of components to evaluate.\n",
    "        \n",
    "    Returns:\n",
    "        list: BIC values for each number of components.\n",
    "    \"\"\"\n",
    "    # List to store BIC values\n",
    "    bic = []\n",
    "    \n",
    "    for n_components in n_components_range:\n",
    "        # Create Gaussian Mixture Model with specified number of components\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        gmm.fit(data)  # Fit the model to the data\n",
    "        bic.append(gmm.bic(data))  # Calculate BIC and add to list\n",
    "        \n",
    "    return bic  # Return list of BIC values\n",
    "\n",
    "n_components_range = range(1, 51)  # Range of number of components to evaluate\n",
    "bic_values = compute_bic(X_train, n_components_range)  # Compute BIC values\n",
    "optimal_n_components = n_components_range[np.argmin(bic_values)]  # Determine optimal number of components\n",
    "\n",
    "# Plotting BIC values\n",
    "plt.plot(n_components_range, bic_values, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('BIC Value')\n",
    "plt.title('BIC for Gaussian Mixture Models')\n",
    "plt.grid(True)\n",
    "# plt.savefig('images/BIC_without_missingness.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Mixture Model with optimal number of components\n",
    "gmm = GaussianMixture(n_components=optimal_n_components, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(cgmm_subsets, 'cgmm', gmm)\n",
    "\n",
    "cgmm_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_score = get_scoring(cgmm_subsets, 'cgmm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_accuracy = get_accuracy(cgmm_subsets, 'cgmm', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['cgmm'] = {'score': cgmm_score, 'accuracy': cgmm_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# # Define function to compute negative log likelihood\n",
    "# def compute_nll(model, X_test):\n",
    "#     reconstructions = model.predict(X_test)\n",
    "#     mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "#     nll = 0.5 * np.log(2 * np.pi * mse)\n",
    "#     return np.mean(nll)\n",
    "\n",
    "# # Define range of latent space dimensionalities to try\n",
    "# latent_dim_range = range(2, 5)\n",
    "\n",
    "# # Train VAE models with different latent space dimensionalities and model capacities\n",
    "# vae_models = {}\n",
    "# for latent_dim in latent_dim_range:\n",
    "#     for num_layers in [1, 2, 3, 4]:\n",
    "#         for num_neurons in [32, 64, 128]:\n",
    "#             # Define the encoder\n",
    "#             input_dim = X_train.shape[1]\n",
    "#             inputs = Input(shape=(input_dim,))\n",
    "#             encoded = inputs\n",
    "#             for i in range(num_layers):\n",
    "#                 encoded = Dense(num_neurons / (2**(i-1)), activation='relu')(encoded)\n",
    "#             z_mean = Dense(latent_dim)(encoded)\n",
    "#             z_log_var = Dense(latent_dim)(encoded)\n",
    "\n",
    "#             # Reparameterization trick\n",
    "#             def sampling(args):\n",
    "#                 z_mean, z_log_var = args\n",
    "#                 epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "#                 return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "#             z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "#             # Define the decoder\n",
    "#             decoded = z\n",
    "#             for i in range(num_layers):\n",
    "#                 decoded = Dense(num_neurons / (2**(i-1)), activation='relu')(decoded)\n",
    "#             outputs = Dense(input_dim)(decoded)\n",
    "\n",
    "#             # Create the VAE model\n",
    "#             vae = Model(inputs, outputs)\n",
    "\n",
    "#             # Compile the model\n",
    "#             vae.compile(optimizer='adam', loss='mse')  # Use MSE as the reconstruction loss\n",
    "\n",
    "#             # Train the model\n",
    "#             history = vae.fit(X_train, X_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "#             # Evaluate performance on test set\n",
    "#             nll_test = compute_nll(vae, X_test)\n",
    "#             print(f\"Latent Dim: {latent_dim}, Num Layers: {num_layers}, Num Neurons: {num_neurons}, Test NLL: {nll_test}\")\n",
    "\n",
    "#             # Store model and its performance\n",
    "#             vae_models[(latent_dim, num_layers, num_neurons)] = {'model': vae, 'nll_test': nll_test}\n",
    "\n",
    "# # Select model with lowest mean NLL on test set\n",
    "# best_config = min(vae_models, key=lambda x: vae_models[x]['nll_test'])\n",
    "# best_model = vae_models[best_config]['model']\n",
    "\n",
    "# print(f\"Best Model Configuration: Latent Dim = {best_config[0]}, Num Layers = {best_config[1]}, Num Neurons = {best_config[2]}, Test NLL = {vae_models[best_config]['nll_test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1641/1641 [==============================] - 4s 2ms/step - loss: 0.1860\n",
      "Epoch 2/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: 0.1323\n",
      "Epoch 3/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: 0.0931\n",
      "Epoch 4/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: 0.0529\n",
      "Epoch 5/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: 0.0660\n",
      "Epoch 6/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: 0.0463\n",
      "Epoch 7/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: 0.0525\n",
      "Epoch 8/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: 0.0359\n",
      "Epoch 9/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: 0.0503\n",
      "Epoch 10/10\n",
      "1641/1641 [==============================] - 3s 2ms/step - loss: 0.0295\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "latent_dim = 4\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "inputs = Input(shape=(input_dim,))\n",
    "encoded = inputs\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "z_mean = Dense(latent_dim)(encoded)\n",
    "z_log_var = Dense(latent_dim)(encoded)\n",
    "\n",
    "# Reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Define the decoder\n",
    "decoded = z\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "outputs = Dense(input_dim)(decoded)\n",
    "\n",
    "# Create the VAE model\n",
    "vae = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "vae.compile(optimizer='adam', loss='mse')  # Use MSE as the reconstruction loss\n",
    "\n",
    "# Train the model\n",
    "history = vae.fit(X_train, X_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(vae_subsets, 'vae', vae)\n",
    "\n",
    "vae_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_score = get_scoring(vae_subsets, 'vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_accuracy = get_accuracy(vae_subsets, 'vae', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['vae'] = {'score': vae_score, 'accuracy': vae_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Convert accuracy values to percentages\n",
    "for key, value in results_dict.items():\n",
    "    results_dict[key][\"accuracy\"] = [round(acc * 100, 2) for acc in value[\"accuracy\"]]\n",
    "\n",
    "# Create a table\n",
    "table = [[\"\"] + list(results_dict.keys())]\n",
    "for i in range(10):\n",
    "    table.append([i+1] + [results_dict[key][\"accuracy\"][i] for key in results_dict.keys()])\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
