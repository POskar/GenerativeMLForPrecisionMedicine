{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RANDOM_STATE = 404\n",
    "number_of_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cardio_train.csv', delimiter=';')\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['cardio'])\n",
    "y = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the results after each method\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Select columns to be scaled\n",
    "numeric_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'gender', 'cholesterol']\n",
    "categorical_columns = ['gluc', 'smoke', 'alco', 'active']\n",
    "\n",
    "# Fit and transform your data (only for numeric columns)\n",
    "scaler = StandardScaler()\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "# Apply one-hot encoding to categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing 10 subsets with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that randomly removes features and replace their values with NaN\n",
    "def remove_features(num_features_to_remove=None, feature_indices_to_remove=None):\n",
    "    \"\"\"\n",
    "    Randomly removes features from a subset of data and replaces their values with NaN.\n",
    "    \n",
    "    Parameters:\n",
    "        num_features_to_remove (int): Number of features to remove randomly.\n",
    "        feature_indices_to_remove (array-like): Indices of features to remove.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Subset of data with removed features and NaN values.\n",
    "    \"\"\"\n",
    "    # Sample a subset of data\n",
    "    subset = X_train.sample(frac=0.001, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Determine features to remove based on number or indices provided\n",
    "    if feature_indices_to_remove is None:\n",
    "        if num_features_to_remove is None:\n",
    "            num_features_to_remove = np.random.randint(1, min(5, len(X_train.columns) - 1))\n",
    "        features_to_remove = np.random.choice(subset.columns[:-1], num_features_to_remove, replace=False)\n",
    "    else:\n",
    "        features_to_remove = subset.columns[feature_indices_to_remove]\n",
    "    \n",
    "    # Replace values of selected features with NaN\n",
    "    features_to_remove = np.random.choice(subset.columns[:-1], num_features_to_remove, replace=False)\n",
    "    subset = subset.astype(object)\n",
    "    subset.loc[:, features_to_remove] = np.nan\n",
    "    \n",
    "    return subset\n",
    "\n",
    "list_of_subsets = []\n",
    "\n",
    "# Generate subsets with varying numbers of removed features\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(1))\n",
    "\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(2))\n",
    "\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(3))\n",
    "\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(4))\n",
    "\n",
    "for _ in range(2):\n",
    "    list_of_subsets.append(remove_features(np.random.randint(5, 7)))\n",
    "\n",
    "# Print information about subsets and their missing columns\n",
    "print(f'Subsets with {list_of_subsets[0].shape[0]} datapoints and their columns with missing values:')\n",
    "for subset_index, subset in enumerate(list_of_subsets):\n",
    "    nan_columns = subset.columns[subset.isnull().all()]\n",
    "    print(f\"Subset {subset_index+1}: {', '.join(nan_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ConditionalGMM.condGMM import CondGMM\n",
    "# import json\n",
    "\n",
    "# def imputing_subsets(subsets, method, model):\n",
    "#     for subset in subsets:\n",
    "#         row_in_subset_index = 0\n",
    "#         for row in subset.iterrows():\n",
    "#             # Get indices and values of unknown and known features\n",
    "#             unknown_features_indexes = [row.index.get_loc(col) for col in row.index if pd.isna(row[col])]\n",
    "            \n",
    "#             # Find indices of known features\n",
    "#             known_features_indexes = list(set(range(subset.shape[1])) - set(unknown_features_indexes))\n",
    "            \n",
    "#             # Extract values of known features for the given row\n",
    "#             known_features_values = subset.iloc[row_in_subset_index, known_features_indexes]\n",
    "            \n",
    "#             # If all features are known, continue\n",
    "#             if len(unknown_features_indexes) == 0:\n",
    "#                 continue\n",
    "            \n",
    "#             sampled_data = None\n",
    "            \n",
    "#             if method == 'simple':\n",
    "#                 # Simple Imputer\n",
    "#                 continue\n",
    "#             elif method == 'multivariate' or method == 'cgmm':\n",
    "#                 # Multivariate Imputer or Conditional GMM\n",
    "#                 sampled_data = cgmm_impute(model, unknown_features_indexes, known_features_values)\n",
    "#                 continue\n",
    "#             elif method == 'vae':\n",
    "#                 # Variational AutoEncoder\n",
    "#                 continue\n",
    "            \n",
    "#             # Update unknown features with sampled data\n",
    "#             for feature_index in range(len(unknown_features_indexes)):\n",
    "#                 if unknown_features_indexes[feature_index] in categorical_columns:\n",
    "#                     # Approximate categorical values to the nearest whole number\n",
    "#                     sampled_data[:, feature_index] = np.round(sampled_data[:, feature_index])\n",
    "#                 subset.iloc[row_in_subset_index, unknown_features_indexes[feature_index]] = json.dumps([sampled_data[sample_index][feature_index] for sample_index in range(sampled_data.shape[0])])\n",
    "                \n",
    "#             row_in_subset_index += 1\n",
    "            \n",
    "# def cgmm_impute(gmm, known_features_indexes, known_features_values):\n",
    "#     # Initialize CondGMM\n",
    "#     cGMM = CondGMM(gmm.weights_, gmm.means_, gmm.covariances_, known_features_indexes)\n",
    "    \n",
    "#     # Generate samples using Conditional GMM\n",
    "#     sampled_data = cGMM.rvs(known_features_values, size=number_of_samples, random_state=RANDOM_STATE)\n",
    "    \n",
    "#     return sampled_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import json\n",
    "\n",
    "def get_scoring(subsets, method, should_print=False):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) scores for features in subsets of data.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Method used for imputation ('simple', 'multivariate', 'cgmm', or 'vae').\n",
    "        should_print (bool): Whether to print MSE scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing MSE scores for each feature in the subsets.\n",
    "    \"\"\"    \n",
    "    # Convert columns to lists if using multivariate or CGMM methods\n",
    "    method = method.lower()\n",
    "    if method == 'multivariate' or method == 'cgmm':\n",
    "        for subset in subsets:\n",
    "            for col in subset.columns:\n",
    "                subset[col] = subset[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    feature_mse = {}  # Initialize dictionary to store MSE values\n",
    "    \n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        # Determine unknown features indexes dynamically for each subset\n",
    "        if method == 'simple':\n",
    "            unknown_features_indexes = [col_index for col_index, col in enumerate(list_of_subsets[subset_index].columns) if list_of_subsets[subset_index][col].isnull().all()]\n",
    "        else:\n",
    "            unknown_features_indexes = [col_index for col_index, col in enumerate(subset.columns) if subset[col].apply(lambda x: isinstance(x, list)).any()]\n",
    "\n",
    "        if not unknown_features_indexes:\n",
    "            continue  # Skip if there are no missing values\n",
    "        \n",
    "        # Iterate through rows in the subset DataFrame\n",
    "        for index, row in subset.iterrows():\n",
    "            original_values = X.iloc[index, unknown_features_indexes].values\n",
    "            \n",
    "            # Compute MSE for each feature separately\n",
    "            for feature_index in range(len(unknown_features_indexes)):\n",
    "                if method == 'simple':\n",
    "                    generated_samples = [row.iloc[unknown_features_indexes].values[feature_index]]\n",
    "                else:    \n",
    "                    generated_samples_raw = row.iloc[unknown_features_indexes].values[feature_index]\n",
    "                    generated_samples = [sample for sample in generated_samples_raw if not pd.isna(sample)]\n",
    "                \n",
    "                # Grab the original value of the feature\n",
    "                original_value = original_values[feature_index]\n",
    "                \n",
    "                for sample in generated_samples:\n",
    "                    original_value_array = np.full_like(np.array(sample), original_value)\n",
    "                    \n",
    "                    mse_value = mean_squared_error(original_value_array.flatten(), np.array(sample).flatten())\n",
    "                    \n",
    "                    if unknown_features_indexes[feature_index] not in feature_mse:\n",
    "                        feature_mse[unknown_features_indexes[feature_index]] = []\n",
    "                        \n",
    "                    feature_mse[unknown_features_indexes[feature_index]].append(mse_value)\n",
    "    \n",
    "        # Print MSE scores if required\n",
    "        if should_print:\n",
    "            print(f\"MSE for Subset {subset_index + 1}:\")\n",
    "            for feature_index, mse_values in feature_mse.items():\n",
    "                mean_mse = np.mean(mse_values)\n",
    "                print(f\"Feature {df.columns[feature_index]}: MSE = {mean_mse}\")\n",
    "    \n",
    "    # Return dictionary containing MSE scores\n",
    "    return feature_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings related to feature names\n",
    "warnings.filterwarnings('ignore', message=\"X does not have valid feature names\")\n",
    "\n",
    "# Load the classifier\n",
    "classifier = load('classifiers\\cardio_classifier.h5')\n",
    "\n",
    "def get_accuracy(subsets, method, should_print=False):\n",
    "    method = method.lower()\n",
    "    if method == 'multivariate' or method == 'cgmm':\n",
    "        # Iterate through each subset and convert strings to lists\n",
    "        for subset in subsets:\n",
    "            for col in subset.columns:\n",
    "                subset[col] = subset[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Initialize an empty list to store classification results and accuracy per subset\n",
    "    classification_results = []\n",
    "    accuracy_per_subset = []\n",
    "\n",
    "    # Iterate through each subset\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        subset_results = []  # Initialize results for this subset\n",
    "        \n",
    "        # Iterate over each row in the subset\n",
    "        for row_index, row in subset.iterrows():\n",
    "            # Initialize an empty list to store results for this row\n",
    "            row_results = []\n",
    "            \n",
    "            if method != 'simple':\n",
    "                serialized_arrays = []\n",
    "                non_serialized_values = []\n",
    "                \n",
    "                # Separate serialized arrays from non-serialized values\n",
    "                for col, value in row.items():\n",
    "                    if isinstance(value, list):\n",
    "                        serialized_arrays.append((col, value))\n",
    "                    else:\n",
    "                        non_serialized_values.append((col, value))\n",
    "                \n",
    "                # Iterate over each index of serialized arrays\n",
    "                for i in range(number_of_samples):\n",
    "                    # Initialize a combined row with non-serialized values\n",
    "                    combined_row = non_serialized_values.copy()\n",
    "                    \n",
    "                    # Append the entry at index i of each serialized array to the combined row\n",
    "                    for col, serialized_array in serialized_arrays:\n",
    "                        if i < len(serialized_array):\n",
    "                            combined_row.append((col, serialized_array[i]))\n",
    "                            \n",
    "                    # Convert combined_row to an array\n",
    "                    combined_row_array = [value for _, value in combined_row]\n",
    "                    \n",
    "                    try:\n",
    "                        # Run the combined row through the classifier\n",
    "                        result_array = classifier.predict([combined_row_array], verbose=0)\n",
    "                        row_results.append(result_array)\n",
    "                    except Exception as e:\n",
    "                        # Handle any potential errors\n",
    "                        print(f\"Error processing row {row_index}: {e}\")\n",
    "            else:\n",
    "                result_array = classifier.predict([row.values.tolist()], verbose=0)\n",
    "                row_results.append(result_array)\n",
    "\n",
    "            # Append the row results to the subset results\n",
    "            subset_results.append(row_results)\n",
    "        \n",
    "        # Append the subset results to the overall results\n",
    "        classification_results.append(subset_results)  \n",
    "\n",
    "\n",
    "    # Iterate through each subset and its corresponding results\n",
    "    for subset_index, subset_results in enumerate(classification_results):\n",
    "        true_labels = y.loc[subsets[subset_index].index]  # Get true labels for the current subset\n",
    "        \n",
    "        # Initialize list to store predicted labels for this subset\n",
    "        subset_predicted_labels = []\n",
    "        \n",
    "        # Iterate through each row and its corresponding results\n",
    "        for row_results in subset_results:\n",
    "            # Get the predicted label for each row (assuming binary classification)\n",
    "            predicted_label = 1 if row_results[0] > 0.5 else 0\n",
    "            subset_predicted_labels.append(predicted_label)\n",
    "        \n",
    "        # Calculate accuracy for this subset\n",
    "        subset_accuracy = accuracy_score(true_labels, subset_predicted_labels)\n",
    "        \n",
    "        # Append the accuracy for this subset to accuracy_per_subset\n",
    "        accuracy_per_subset.append(subset_accuracy)\n",
    "\n",
    "\n",
    "    if (should_print):\n",
    "        # Print accuracy per subset\n",
    "        for subset_index, subset_accuracy in enumerate(accuracy_per_subset):\n",
    "            print(\"Subset\", subset_index+1, \"accuracy:\", subset_accuracy)\n",
    "        \n",
    "    return accuracy_per_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleImputer with mean strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "imputer_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "for subset in imputer_subsets:\n",
    "    for col in subset.columns:\n",
    "        if pd.isna(subset[col]).any():\n",
    "            imp.fit(X_train[[col]])\n",
    "            subset[col] = imp.transform(subset[[col]])\n",
    "            \n",
    "imputer_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer_score = get_scoring(imputer_subsets, 'simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer_accuracy = get_accuracy(imputer_subsets, 'simple', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['simple_imputer'] = {'score': simple_imputer_score, 'accuracy': simple_imputer_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create Gaussian Mixture Model with a single component\n",
    "gmm = GaussianMixture(n_components=1, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConditionalGMM.condGMM import CondGMM\n",
    "\n",
    "def cgmm_generate_samples(subsets, gmm_weights, gmm_means, gmm_covariances):\n",
    "    for subset in subsets:\n",
    "        index = 0\n",
    "        for row_index, row in subset.iterrows():\n",
    "            # Get indices and values of unknown and known features\n",
    "            unknown_features_indexes = [row.index.get_loc(col) for col in row.index if pd.isna(row[col])]\n",
    "            \n",
    "            # Find indices of known features\n",
    "            known_features_indexes = list(set(range(subset.shape[1])) - set(unknown_features_indexes))\n",
    "            \n",
    "            # Extract values of known features for the given row\n",
    "            known_features_values = subset.iloc[index, known_features_indexes]\n",
    "            \n",
    "            # If all features are known, continue\n",
    "            if len(unknown_features_indexes) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Initialize CondGMM\n",
    "            cGMM = CondGMM(gmm_weights, gmm_means, gmm_covariances, known_features_indexes)\n",
    "            \n",
    "            # Generate samples using Conditional GMM\n",
    "            sampled_data = cGMM.rvs(known_features_values, size=number_of_samples, random_state=RANDOM_STATE)\n",
    "            \n",
    "            # Update unknown features with sampled data\n",
    "            for feature_index in range(len(unknown_features_indexes)):\n",
    "                if unknown_features_indexes[feature_index] in categorical_columns:\n",
    "                    # Approximate categorical values to the nearest whole number\n",
    "                    sampled_data[:, feature_index] = np.round(sampled_data[:, feature_index])\n",
    "                subset.iloc[index, unknown_features_indexes[feature_index]] = json.dumps([sampled_data[sample_index][feature_index] for sample_index in range(sampled_data.shape[0])])\n",
    "                \n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_generate_samples(multivariate_subsets, gmm.weights_, gmm.means_, gmm.covariances_)\n",
    "\n",
    "multivariate_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_score = get_scoring(multivariate_subsets, 'multivariate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_accuracy = get_accuracy(multivariate_subsets, 'multivariate', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['multivariate'] = {'score': multivariate_score, 'accuracy': multivariate_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BIC to get the optimal number of components for GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_bic(data, n_components_range):\n",
    "    \"\"\"\n",
    "    Computes the Bayesian Information Criterion (BIC) for Gaussian Mixture Models with different numbers of components.\n",
    "    \n",
    "    Parameters:\n",
    "        X (array-like): Input data.\n",
    "        n_components_range (range): Range of number of components to evaluate.\n",
    "        \n",
    "    Returns:\n",
    "        list: BIC values for each number of components.\n",
    "    \"\"\"\n",
    "    bic = []  # List to store BIC values\n",
    "    for n_components in n_components_range:\n",
    "        # Create Gaussian Mixture Model with specified number of components\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        gmm.fit(data)  # Fit the model to the data\n",
    "        bic.append(gmm.bic(data))  # Calculate BIC and add to list\n",
    "    return bic  # Return list of BIC values\n",
    "\n",
    "n_components_range = range(1, 51)  # Range of number of components to evaluate\n",
    "bic_values = compute_bic(X_train, n_components_range)  # Compute BIC values\n",
    "optimal_n_components = n_components_range[np.argmin(bic_values)]  # Determine optimal number of components\n",
    "\n",
    "# Plotting BIC values\n",
    "plt.plot(n_components_range, bic_values, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('BIC Value')\n",
    "plt.title('BIC for Gaussian Mixture Models')\n",
    "plt.grid(True)\n",
    "plt.savefig('images/BIC_without_missingness.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Mixture Model with optimal number of components\n",
    "gmm = GaussianMixture(n_components=optimal_n_components, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_generate_samples(cgmm_subsets, gmm.weights_, gmm.means_, gmm.covariances_)\n",
    "\n",
    "cgmm_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_score = get_scoring(cgmm_subsets, 'cgmm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_accuracy = get_accuracy(cgmm_subsets, 'cgmm', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['cgmm'] = {'score': cgmm_score, 'accuracy': cgmm_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define function to compute negative log likelihood\n",
    "def compute_nll(model, X_test):\n",
    "    reconstructions = model.predict(X_test)\n",
    "    mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "    nll = 0.5 * np.log(2 * np.pi * mse)\n",
    "    return np.mean(nll)\n",
    "\n",
    "# Define range of latent space dimensionalities to try\n",
    "latent_dim_range = range(2, 5)\n",
    "\n",
    "# Train VAE models with different latent space dimensionalities and model capacities\n",
    "vae_models = {}\n",
    "for latent_dim in latent_dim_range:\n",
    "    for num_layers in [1, 2, 3]:\n",
    "        for num_neurons in [32, 64]:\n",
    "            # Define the encoder\n",
    "            input_dim = X_train.shape[1]\n",
    "            inputs = Input(shape=(input_dim,))\n",
    "            encoded = inputs\n",
    "            for _ in range(num_layers):\n",
    "                encoded = Dense(num_neurons, activation='relu')(encoded)\n",
    "            z_mean = Dense(latent_dim)(encoded)\n",
    "            z_log_var = Dense(latent_dim)(encoded)\n",
    "\n",
    "            # Reparameterization trick\n",
    "            def sampling(args):\n",
    "                z_mean, z_log_var = args\n",
    "                epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "                return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "            z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "            # Define the decoder\n",
    "            decoded = z\n",
    "            for _ in range(num_layers):\n",
    "                decoded = Dense(num_neurons, activation='relu')(decoded)\n",
    "            outputs = Dense(input_dim)(decoded)\n",
    "\n",
    "            # Create the VAE model\n",
    "            vae = Model(inputs, outputs)\n",
    "\n",
    "            # Compile the model\n",
    "            vae.compile(optimizer='adam', loss='mse')  # Use MSE as the reconstruction loss\n",
    "\n",
    "            # Train the model\n",
    "            history = vae.fit(X_train, X_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "            # Evaluate performance on test set\n",
    "            nll_test = compute_nll(vae, X_test)\n",
    "            print(f\"Latent Dim: {latent_dim}, Num Layers: {num_layers}, Num Neurons: {num_neurons}, Test NLL: {nll_test}\")\n",
    "\n",
    "            # Store model and its performance\n",
    "            vae_models[(latent_dim, num_layers, num_neurons)] = {'model': vae, 'nll_test': nll_test}\n",
    "\n",
    "# Select model with lowest mean NLL on test set\n",
    "best_config = min(vae_models, key=lambda x: vae_models[x]['nll_test'])\n",
    "best_model = vae_models[best_config]['model']\n",
    "\n",
    "print(f\"Best Model Configuration: Latent Dim = {best_config[0]}, Num Layers = {best_config[1]}, Num Neurons = {best_config[2]}, Test NLL = {vae_models[best_config]['nll_test']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_subsets = copy.deepcopy(list_of_subsets)\n",
    "\n",
    "# Iterate through each subset\n",
    "for subset_index, subset in enumerate(vae_subsets):\n",
    "    index = 0\n",
    "    # Determine unknown features indexes dynamically for each subset\n",
    "    unknown_features_indexes = np.where(subset.isnull().any())[0]\n",
    "\n",
    "    # Iterate through each row\n",
    "    for row_index, row in subset.iterrows():\n",
    "        sampled_data = np.empty((number_of_samples, len(unknown_features_indexes)))\n",
    "        \n",
    "        # Repeat the prediction process for the specified number of samples\n",
    "        for _ in range(number_of_samples):\n",
    "            imputed_values_row = []\n",
    "            # Impute missing values for each feature index\n",
    "            for feature_index in unknown_features_indexes:\n",
    "                # Impute missing value using the VAE for the current feature and row\n",
    "                imputed_value = best_model.predict(row.values.reshape(1, -1).astype(np.float32), verbose=0)[0, feature_index]\n",
    "                imputed_values_row.append(imputed_value)\n",
    "            sampled_data[_] = imputed_values_row\n",
    "         \n",
    "        for feature_index in range(len(unknown_features_indexes)):\n",
    "            if unknown_features_indexes[feature_index] in categorical_columns:\n",
    "                # Approximate categorical values to the nearest whole number\n",
    "                sampled_data[:, feature_index] = np.round(sampled_data[:, feature_index])\n",
    "            subset.iloc[index, unknown_features_indexes[feature_index]] = json.dumps([sampled_data[sample_index][feature_index] for sample_index in range(sampled_data.shape[0])])\n",
    "\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_score = get_scoring(vae_subsets, 'vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_accuracy = get_accuracy(vae_subsets, 'vae', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['vae'] = {'score': vae_score, 'accuracy': vae_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Convert accuracy values to percentages\n",
    "for key, value in results_dict.items():\n",
    "    results_dict[key][\"accuracy\"] = [round(acc * 100, 2) for acc in value[\"accuracy\"]]\n",
    "\n",
    "# Create a table\n",
    "table = [[\"\"] + list(results_dict.keys())]\n",
    "for i in range(10):\n",
    "    table.append([i+1] + [results_dict[key][\"accuracy\"][i] for key in results_dict.keys()])\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
