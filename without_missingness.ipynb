{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_STATE = 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cardio_train.csv', delimiter=';')\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['cardio'])\n",
    "y = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the results after each method\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Select columns to be scaled\n",
    "numeric_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'gender', 'cholesterol']\n",
    "categorical_columns = ['gluc', 'smoke', 'alco', 'active']\n",
    "\n",
    "# Fit and transform your data (only for numeric columns)\n",
    "scaler = StandardScaler()\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "# Apply one-hot encoding to categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Loop through each column in X and generate a density plot\n",
    "for i, feature_name in enumerate(df.columns):\n",
    "    # Set the subplot and plot the density of the column\n",
    "    plt.subplot(4, 4, i + 1)  # 4x4 grid, current subplot index\n",
    "    df[feature_name].plot(kind='density', color='blue', label=feature_name)\n",
    "    plt.title(feature_name)\n",
    "    plt.xlabel('Scaled Value')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "# Adjust the layout of the subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as an image\n",
    "plt.savefig('images/without_missingness/density_plots.png')\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw correlation matrix\n",
    "sns.heatmap(X.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "# Show the figure\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Save the figure as an image\n",
    "plt.savefig('images/without_missingness/correlation_matrix.png')\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing 10 subsets with removed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_of_data = 0.001\n",
    "\n",
    "def remove_features(num_features_to_remove=None, feature_indices_to_remove=None):\n",
    "    \"\"\"\n",
    "    Randomly removes features from a subset of data and replaces their values with NaN.\n",
    "    \n",
    "    Parameters:\n",
    "        num_features_to_remove (int): Number of features to remove randomly.\n",
    "        feature_indices_to_remove (array-like): Indices of features to remove.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Subset of data with removed features and NaN values.\n",
    "    \"\"\"\n",
    "    # Sample a subset of data\n",
    "    subset = X_train.sample(frac=fraction_of_data, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Determine features to remove based on number or indices provided\n",
    "    if feature_indices_to_remove is None:\n",
    "        if num_features_to_remove is None:\n",
    "            num_features_to_remove = np.random.randint(1, min(5, len(X_train.columns) - 1))\n",
    "        features_to_remove = np.random.choice(subset.columns[:-1], num_features_to_remove, replace=False)\n",
    "    else:\n",
    "        features_to_remove = subset.columns[feature_indices_to_remove]\n",
    "    \n",
    "    # Replace values of selected features with NaN\n",
    "    features_to_remove = np.random.choice(subset.columns[:-1], num_features_to_remove, replace=False)\n",
    "    subset = subset.astype(object)\n",
    "    subset.loc[:, features_to_remove] = np.nan\n",
    "    \n",
    "    return subset\n",
    "\n",
    "list_of_subsets = []\n",
    "subset_without_changes = X_train.sample(frac=fraction_of_data, random_state=RANDOM_STATE)\n",
    "\n",
    "# Generate subsets with varying numbers of removed features\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(1))\n",
    "\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(2))\n",
    "\n",
    "for _ in range(3):\n",
    "    list_of_subsets.append(remove_features(3))\n",
    "\n",
    "for _ in range(1):\n",
    "    list_of_subsets.append(remove_features(4))\n",
    "\n",
    "# Print information about subsets and their missing columns\n",
    "print(f'Subsets with {list_of_subsets[0].shape[0]} datapoints and their columns with missing values:')\n",
    "for subset_index, current_row in enumerate(list_of_subsets):\n",
    "    nan_columns = current_row.columns[current_row.isnull().all()]\n",
    "    print(f\"Subset {subset_index+1}: {', '.join(nan_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from ConditionalGMM.condGMM import CondGMM\n",
    "import json\n",
    "\n",
    "def imputing_missing_data(subsets, method='simple', number_of_samples=10, model=None):\n",
    "    \"\"\"\n",
    "    Impute missing data in subsets using different imputation methods.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Imputation method ('simple', 'multivariate', 'cgmm', 'vae', or 'gain').\n",
    "        model: Trained model for certain imputation methods.\n",
    "    \"\"\"\n",
    "    for subset in subsets:\n",
    "        if method == 'simple':\n",
    "            # Simple Imputer\n",
    "            generated_data = simple_impute(subset)\n",
    "            continue\n",
    "        \n",
    "        # Initialize to keep track of actual row index, because indices were shuffled\n",
    "        row_in_subset_index = 0\n",
    "        \n",
    "        for row_index, row in subset.iterrows():\n",
    "            # Get indices of unknown features\n",
    "            missing_features_indices = [row.index.get_loc(col) for col in row.index if pd.isna(row[col])]\n",
    "            \n",
    "            # If all features are known, continue\n",
    "            if len(missing_features_indices) == 0:\n",
    "                continue\n",
    "            \n",
    "            generated_data = None\n",
    "            \n",
    "            if method == 'multivariate' or method == 'cgmm':\n",
    "                # Multivariate Imputer or Conditional GMM\n",
    "                generated_data = cgmm_impute(model, missing_features_indices, row, number_of_samples)\n",
    "            elif method == 'vae':\n",
    "                # Variational AutoEncoder\n",
    "                generated_data = vae_impute(model, missing_features_indices, row, number_of_samples)\n",
    "            elif method == 'gain':\n",
    "                # Generative Adv\n",
    "                generated_data = gain_impute(model, missing_features_indices, row, number_of_samples)\n",
    "                \n",
    "            # Update unknown features with sampled data\n",
    "            for feature_index in range(len(missing_features_indices)):\n",
    "                if subset.columns[missing_features_indices[feature_index]] in categorical_columns:\n",
    "                    # Approximate categorical values to the nearest whole number\n",
    "                    generated_data[:, feature_index] = np.round(generated_data[:, feature_index])\n",
    "                # subset.iloc[row_in_subset_index, missing_features_indices[feature_index]] = json.dumps([generated_data[sample_index][feature_index] for sample_index in range(generated_data.shape[0])])\n",
    "                subset.iloc[row_in_subset_index, missing_features_indices[feature_index]] = generated_data[:, feature_index]\n",
    "            \n",
    "            row_in_subset_index += 1\n",
    "\n",
    "def simple_impute(current_subset):\n",
    "    \"\"\"\n",
    "    Impute missing values using SimpleImputer with mean strategy.\n",
    "\n",
    "    Parameters:\n",
    "        current_subset (pandas.DataFrame): Subset of data with missing values.\n",
    "\n",
    "    This function iterates over each column in the given DataFrame and imputes missing values using\n",
    "    the SimpleImputer class from scikit-learn. The imputer is initialized with the 'mean' strategy,\n",
    "    which replaces missing values with the mean of the non-missing values in the column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a SimpleImputer object with 'mean' strategy\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col in current_subset.columns:\n",
    "        # Check if any values in the column are missing\n",
    "        if pd.isna(current_subset[col]).any():\n",
    "            # Fit the imputer to the non-missing values in the column\n",
    "            imp.fit(X_train[[col]])\n",
    "            # Transform the missing values in the column\n",
    "            current_subset[col] = imp.transform(current_subset[[col]])\n",
    "\n",
    "def cgmm_impute(gmm, missing_features_indices, current_row, number_of_samples):\n",
    "    \"\"\"\n",
    "    Impute missing values using Conditional GMM.\n",
    "    \n",
    "    Parameters:\n",
    "        gmm (GMM): Gaussian Mixture Model.\n",
    "        missing_features_indices (list): Indices of missing features.\n",
    "        current_row (pandas.Series): Current row with missing values.\n",
    "        number_of_samples (int): Number of samples to generate.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Generated data with imputed missing values.\n",
    "    \"\"\"\n",
    "    # Find indices of known features\n",
    "    known_features_indices = [i for i in range(len(current_row))  # Iterate over indices\n",
    "                               if i not in missing_features_indices]  # Check if index is in missing_features_indices\n",
    "    \n",
    "    # Extract values of known features for the given row\n",
    "    known_features_values = current_row[known_features_indices]\n",
    "    \n",
    "    # Initialize CondGMM\n",
    "    cGMM = CondGMM(gmm.weights_, gmm.means_, gmm.covariances_, known_features_indices)\n",
    "    \n",
    "    # Generate samples using Conditional GMM\n",
    "    generated_data = cGMM.rvs(known_features_values, size=number_of_samples, random_state=RANDOM_STATE)\n",
    "    \n",
    "    return generated_data\n",
    "\n",
    "def vae_impute(model, missing_features_indices, current_row, number_of_samples):\n",
    "    \"\"\"\n",
    "    Impute missing values using a Variational Autoencoder.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Trained Variational Autoencoder model.\n",
    "        missing_features_indices (list): Indices of missing features.\n",
    "        current_row (pandas.Series): Current row with missing values.\n",
    "        number_of_samples (int): Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: Generated data with imputed missing values.\n",
    "    \"\"\"\n",
    "    generated_data = np.empty((number_of_samples, len(missing_features_indices)))\n",
    "    \n",
    "    # Repeat the prediction process for the specified number of samples\n",
    "    for _ in range(number_of_samples):\n",
    "        # Impute missing values for each feature index\n",
    "        # Reshape the missing values to a 2D array with one row and all missing features\n",
    "        missing_features_values = model.predict(current_row.values.reshape(1, -1).astype(np.float32), verbose=0)\n",
    "        \n",
    "        # Append the generated data to the list\n",
    "        generated_data = np.append(generated_data, missing_features_values[:, missing_features_indices], axis=0)\n",
    "    \n",
    "    return generated_data\n",
    "\n",
    "def gain_impute(model, missing_features_indices, current_row, number_of_samples):\n",
    "    \"\"\"\n",
    "    Impute missing values using a Generative Adversarial Imputation Network.\n",
    "\n",
    "    Args:\n",
    "        model (keras.Model): Trained GAIN model.\n",
    "        missing_features_indices (list): Indices of missing features.\n",
    "        current_row (pandas.Series): Current row with missing values.\n",
    "        number_of_samples (int): Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: Generated data with imputed missing values.\n",
    "    \"\"\"\n",
    "    generated_data = []\n",
    "    \n",
    "    # Repeat the prediction process for the specified number of samples\n",
    "    for _ in range(number_of_samples):\n",
    "        # Impute missing values for each feature index\n",
    "        # Reshape the missing values to a 2D array with one row and all missing features\n",
    "        missing_features_values = model.predict(current_row.values.reshape(1, -1).astype(np.float64), verbose=0)\n",
    "        \n",
    "        # Append the generated data to the list\n",
    "        generated_data.append(missing_features_values[:, missing_features_indices])\n",
    "        \n",
    "    # Convert the list of arrays to a numpy array\n",
    "    generated_data = np.concatenate(generated_data, axis=0)\n",
    "        \n",
    "    return generated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scoring(subsets, method='simple', print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) scores for features in subsets of data.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Method used for imputation ('simple', 'multivariate', 'cgmm', or 'vae').\n",
    "        print_results (bool): Whether to print MSE scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing MSE scores for each feature in the subsets.\n",
    "    \"\"\"    \n",
    "    method = method.lower()\n",
    "    \n",
    "    # Initialize dictionary to store scores\n",
    "    all_subsets_scores = {}\n",
    "    \n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        feature_score = {}  # Initialize dictionary to store MSE values\n",
    "        \n",
    "        # Determine unknown features indexes dynamically for each subset\n",
    "        if method == 'simple':\n",
    "            missing_features_indices = [col_index for col_index, col in enumerate(list_of_subsets[subset_index].columns) if list_of_subsets[subset_index][col].isnull().all()]\n",
    "        else:\n",
    "            missing_features_indices = [col_index for col_index, col in enumerate(subset.columns) if subset[col].apply(lambda x: isinstance(x, np.ndarray)).any()]\n",
    "\n",
    "        if not missing_features_indices:\n",
    "            continue  # Skip if there are no missing values\n",
    "        \n",
    "        # Iterate through rows in the subset DataFrame\n",
    "        for index, row in subset.iterrows():\n",
    "            original_values = X.iloc[index, missing_features_indices].values\n",
    "            \n",
    "            # Compute MSE for each feature separately\n",
    "            for feature_index in range(len(missing_features_indices)):\n",
    "                if method == 'simple':\n",
    "                    generated_samples = [row.iloc[missing_features_indices].values[feature_index]]\n",
    "                else:\n",
    "                    try:\n",
    "                        generated_samples_raw = row.iloc[missing_features_indices].values[feature_index]\n",
    "                        generated_samples = [sample for sample in generated_samples_raw if not pd.isna(sample)]\n",
    "                    except Exception as e:\n",
    "                        # TODO: Add handling for exceptions\n",
    "                        print(f\"Error processing row {index}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Grab the original value of the feature\n",
    "                original_value = original_values[feature_index]\n",
    "                \n",
    "                for sample in generated_samples:\n",
    "                    squared_error = (original_value - sample)**2\n",
    "                    \n",
    "                    if missing_features_indices[feature_index] not in feature_score:\n",
    "                        feature_score[missing_features_indices[feature_index]] = []\n",
    "                        \n",
    "                    feature_score[missing_features_indices[feature_index]].append(squared_error)\n",
    "        \n",
    "        for feature_index, score_values in feature_score.items():\n",
    "            mse = np.mean(score_values)\n",
    "            \n",
    "            # TODO: Not sure if it should be X, X_train, or subset_without_changes\n",
    "            variance = np.var(subset_without_changes.iloc[:, feature_index])\n",
    "            nmse = mse / variance\n",
    "            feature_score[feature_index] = np.round(nmse, 3)\n",
    "            \n",
    "        all_subsets_scores[subset_index] = feature_score\n",
    "        \n",
    "        # Print MSE scores if required\n",
    "        if print_results:\n",
    "            print(f\"MSE for Subset {subset_index + 1}:\")\n",
    "            for feature_index, score_values in feature_score.items():\n",
    "                print(f\"Feature {subset_without_changes.columns[feature_index]}: MSE = {score_values}\")\n",
    "    \n",
    "    # Return dictionary containing MSE scores\n",
    "    return all_subsets_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', message=\"X does not have valid feature names\")\n",
    "\n",
    "# Load the trained classifier model\n",
    "classifier = load('classifiers\\cardio_classifier.h5')\n",
    "\n",
    "def get_accuracy(subsets, method='simple', should_print=False):\n",
    "    \"\"\"\n",
    "    Calculate accuracy scores for subsets of data using a trained classifier.\n",
    "    \n",
    "    Parameters:\n",
    "        subsets (list): List of subsets of data.\n",
    "        method (str): Method used for imputation ('simple', 'multivariate', or 'cgmm').\n",
    "        should_print (bool): Whether to print accuracy scores or not.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of accuracy scores for each subset.\n",
    "    \"\"\"\n",
    "    method = method.lower()\n",
    "    \n",
    "    classification_results = []  # Initialize list to store classification results\n",
    "    accuracy_per_subset = []  # Initialize list to store accuracy scores\n",
    "\n",
    "    # Iterate through subsets\n",
    "    for subset_index, subset in enumerate(subsets):\n",
    "        subset_results = []  # Initialize list to store results for the current subset\n",
    "        \n",
    "        # Iterate through rows in the subset DataFrame\n",
    "        for row_index, row in subset.iterrows():\n",
    "            row_results = []  # Initialize list to store results for the current row\n",
    "            \n",
    "            # Process each row based on the method used\n",
    "            if method != 'simple':\n",
    "                serialized_arrays = []\n",
    "                non_serialized_values = []\n",
    "                \n",
    "                number_of_samples = 1\n",
    "                \n",
    "                # Split row values into serialized arrays and non-serialized values\n",
    "                for col, value in row.items():\n",
    "                    if isinstance(value, np.ndarray):\n",
    "                        serialized_arrays.append((col, value))\n",
    "                        if number_of_samples < len(value):\n",
    "                            number_of_samples = len(value)\n",
    "                    else:\n",
    "                        non_serialized_values.append((col, value))\n",
    "                \n",
    "                # Generate combined rows by combining serialized arrays with non-serialized values\n",
    "                for i in range(number_of_samples):\n",
    "                    combined_row = non_serialized_values.copy()\n",
    "                    \n",
    "                    for col, serialized_array in serialized_arrays:\n",
    "                        if i < len(serialized_array):\n",
    "                            combined_row.append((col, serialized_array[i]))\n",
    "                    \n",
    "                    combined_row_array = [value for _, value in combined_row]\n",
    "                    \n",
    "                    try:\n",
    "                        result_array = classifier.predict([combined_row_array], verbose=0)\n",
    "                        row_results.append(result_array)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing row {row_index}: {e}\")\n",
    "            else:\n",
    "                # For simple method, predict directly from row values\n",
    "                result_array = classifier.predict([row.values.tolist()], verbose=0)\n",
    "                row_results.append(result_array)\n",
    "\n",
    "            subset_results.append(row_results)  # Append results for the current row to the subset results\n",
    "        \n",
    "        classification_results.append(subset_results)  # Append subset results to the classification results\n",
    "\n",
    "    # Calculate accuracy scores for each subset\n",
    "    for subset_index, subset_results in enumerate(classification_results):\n",
    "        true_labels = y.loc[subsets[subset_index].index]  # Get true labels for the current subset\n",
    "        \n",
    "        subset_predicted_labels = []  # Initialize list to store predicted labels for the subset\n",
    "        \n",
    "        # Determine predicted labels for each row in the subset\n",
    "        for row_results in subset_results:\n",
    "            predicted_label = 1 if row_results[0] > 0.5 else 0  # Assuming threshold of 0.5\n",
    "            subset_predicted_labels.append(predicted_label)\n",
    "        \n",
    "        subset_accuracy = accuracy_score(true_labels, subset_predicted_labels)  # Calculate accuracy score for the subset\n",
    "        accuracy_per_subset.append(round(subset_accuracy * 100, 2))  # Append accuracy score to the list\n",
    "\n",
    "    # Print accuracy scores if required\n",
    "    if (should_print):\n",
    "        for subset_index, subset_accuracy in enumerate(accuracy_per_subset):\n",
    "            print(\"Subset\", subset_index+1, \"accuracy:\", subset_accuracy)\n",
    "        \n",
    "    return accuracy_per_subset  # Return list of accuracy scores for each subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleImputer with mean strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "imputer_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40709</th>\n",
       "      <td>1.233014</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>1.296064</td>\n",
       "      <td>0.402504</td>\n",
       "      <td>0.137541</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089036</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28468</th>\n",
       "      <td>0.061662</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>-1.261771</td>\n",
       "      <td>-0.083754</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089036</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14340</th>\n",
       "      <td>-0.509423</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>0.199849</td>\n",
       "      <td>-0.153219</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.088238</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089036</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20042</th>\n",
       "      <td>-2.039071</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>-0.652763</td>\n",
       "      <td>-1.125735</td>\n",
       "      <td>-0.187113</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089036</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55213</th>\n",
       "      <td>-1.589985</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>0.565254</td>\n",
       "      <td>-0.639477</td>\n",
       "      <td>-0.122182</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089036</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    gender    height    weight     ap_hi     ap_lo cholesterol  \\\n",
       "40709  1.233014 -0.733108  1.296064  0.402504  0.137541  0.017879   -0.539322   \n",
       "28468  0.061662 -0.733108 -1.261771 -0.083754  0.007679 -0.141297   -0.539322   \n",
       "14340 -0.509423 -0.733108  0.199849 -0.153219 -0.057251 -0.088238   -0.539322   \n",
       "20042 -2.039071 -0.733108 -0.652763 -1.125735 -0.187113 -0.141297   -0.539322   \n",
       "55213 -1.589985 -0.733108  0.565254 -0.639477 -0.122182 -0.141297   -0.539322   \n",
       "\n",
       "      gluc     smoke alco active  \n",
       "40709    0  0.089036    0      0  \n",
       "28468    0  0.089036    0      0  \n",
       "14340    0  0.089036    0      1  \n",
       "20042    0  0.089036    0      1  \n",
       "55213    0  0.089036    0      1  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputing_missing_data(imputer_subsets, 'simple')\n",
    "\n",
    "imputer_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Subset 1:\n",
      "Feature smoke: MSE = 1.083\n",
      "MSE for Subset 2:\n",
      "Feature weight: MSE = 1.01\n",
      "MSE for Subset 3:\n",
      "Feature age: MSE = 1.019\n",
      "MSE for Subset 4:\n",
      "Feature ap_lo: MSE = 1.685\n",
      "Feature cholesterol: MSE = 1.004\n",
      "MSE for Subset 5:\n",
      "Feature gluc: MSE = 1.009\n",
      "Feature smoke: MSE = 1.083\n",
      "MSE for Subset 6:\n",
      "Feature weight: MSE = 1.01\n",
      "Feature gluc: MSE = 1.009\n",
      "MSE for Subset 7:\n",
      "Feature age: MSE = 1.019\n",
      "Feature ap_hi: MSE = 1.062\n",
      "Feature cholesterol: MSE = 1.004\n",
      "MSE for Subset 8:\n",
      "Feature age: MSE = 1.019\n",
      "Feature ap_lo: MSE = 1.685\n",
      "Feature gluc: MSE = 1.009\n",
      "MSE for Subset 9:\n",
      "Feature ap_lo: MSE = 1.685\n",
      "Feature gluc: MSE = 1.009\n",
      "Feature alco: MSE = 1.0\n",
      "MSE for Subset 10:\n",
      "Feature age: MSE = 1.019\n",
      "Feature ap_lo: MSE = 1.685\n",
      "Feature smoke: MSE = 1.083\n",
      "Feature alco: MSE = 1.0\n"
     ]
    }
   ],
   "source": [
    "simple_imputer_score = get_scoring(imputer_subsets, 'simple', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 1 accuracy: 73.21\n",
      "Subset 2 accuracy: 73.21\n",
      "Subset 3 accuracy: 69.64\n",
      "Subset 4 accuracy: 73.21\n",
      "Subset 5 accuracy: 73.21\n",
      "Subset 6 accuracy: 73.21\n",
      "Subset 7 accuracy: 60.71\n",
      "Subset 8 accuracy: 73.21\n",
      "Subset 9 accuracy: 73.21\n",
      "Subset 10 accuracy: 73.21\n"
     ]
    }
   ],
   "source": [
    "simple_imputer_accuracy = get_accuracy(imputer_subsets, 'simple', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['simple_imputer'] = {'score': simple_imputer_score, 'accuracy': np.round(simple_imputer_accuracy, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(random_state=404)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GaussianMixture<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.mixture.GaussianMixture.html\">?<span>Documentation for GaussianMixture</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GaussianMixture(random_state=404)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "GaussianMixture(random_state=404)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create Gaussian Mixture Model with a single component\n",
    "gmm = GaussianMixture(n_components=1, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pieni\\AppData\\Local\\Temp\\ipykernel_8340\\982272638.py:95: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  known_features_values = current_row[known_features_indices]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40709</th>\n",
       "      <td>1.233014</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>1.296064</td>\n",
       "      <td>0.402504</td>\n",
       "      <td>0.137541</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28468</th>\n",
       "      <td>0.061662</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>-1.261771</td>\n",
       "      <td>-0.083754</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14340</th>\n",
       "      <td>-0.509423</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>0.199849</td>\n",
       "      <td>-0.153219</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.088238</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20042</th>\n",
       "      <td>-2.039071</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>-0.652763</td>\n",
       "      <td>-1.125735</td>\n",
       "      <td>-0.187113</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55213</th>\n",
       "      <td>-1.589985</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>0.565254</td>\n",
       "      <td>-0.639477</td>\n",
       "      <td>-0.122182</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    gender    height    weight     ap_hi     ap_lo cholesterol  \\\n",
       "40709  1.233014 -0.733108  1.296064  0.402504  0.137541  0.017879   -0.539322   \n",
       "28468  0.061662 -0.733108 -1.261771 -0.083754  0.007679 -0.141297   -0.539322   \n",
       "14340 -0.509423 -0.733108  0.199849 -0.153219 -0.057251 -0.088238   -0.539322   \n",
       "20042 -2.039071 -0.733108 -0.652763 -1.125735 -0.187113 -0.141297   -0.539322   \n",
       "55213 -1.589985 -0.733108  0.565254 -0.639477 -0.122182 -0.141297   -0.539322   \n",
       "\n",
       "      gluc                                              smoke alco active  \n",
       "40709    0  [-0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0...    0      0  \n",
       "28468    0  [-0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0...    0      0  \n",
       "14340    0  [-0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0...    0      1  \n",
       "20042    0  [-0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0...    0      1  \n",
       "55213    0  [-0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0...    0      1  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputing_missing_data(multivariate_subsets, 'multivariate', 10, gmm)\n",
    "\n",
    "multivariate_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_score = get_scoring(multivariate_subsets, 'multivariate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 1 accuracy: 75.0\n",
      "Subset 2 accuracy: 55.36\n",
      "Subset 3 accuracy: 53.57\n",
      "Subset 4 accuracy: 69.64\n",
      "Subset 5 accuracy: 69.64\n",
      "Subset 6 accuracy: 55.36\n",
      "Subset 7 accuracy: 53.57\n",
      "Subset 8 accuracy: 46.43\n",
      "Subset 9 accuracy: 67.86\n",
      "Subset 10 accuracy: 53.57\n"
     ]
    }
   ],
   "source": [
    "multivariate_accuracy = get_accuracy(multivariate_subsets, 'multivariate', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['multivariate'] = {'score': multivariate_score, 'accuracy': np.round(multivariate_accuracy * 100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_bic(data, n_components_range):\n",
    "    \"\"\"\n",
    "    Computes the Bayesian Information Criterion (BIC) for Gaussian Mixture Models\n",
    "    with different numbers of components.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): Input data.\n",
    "        n_components_range (range): Range of number of components to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        list: BIC values for each number of components.\n",
    "    \"\"\"\n",
    "    # List to store BIC values\n",
    "    bic = []\n",
    "    \n",
    "    # Loop through number of components and compute BIC for each\n",
    "    for n_components in n_components_range:\n",
    "        # Create Gaussian Mixture Model with specified number of components\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=RANDOM_STATE)\n",
    "        gmm.fit(data)  # Fit the model to the data\n",
    "        bic.append(gmm.bic(data))  # Calculate BIC and add to list\n",
    "    \n",
    "    return bic  # Return list of BIC values\n",
    "\n",
    "# Range of number of components to evaluate\n",
    "n_components_range = range(1, 31)\n",
    "\n",
    "# Compute BIC values\n",
    "bic_values = compute_bic(df.drop(columns=['cardio']), n_components_range)\n",
    "\n",
    "# Optimal number of components based on elbow point\n",
    "optimal_n_components = n_components_range[np.argmin(bic_values)]\n",
    "\n",
    "# Plotting BIC values\n",
    "plt.plot(n_components_range, bic_values, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('BIC Value')\n",
    "plt.title('BIC for Gaussian Mixture Models')\n",
    "plt.grid(True)\n",
    "\n",
    "# Highlighting the elbow point\n",
    "plt.axvline(x=optimal_n_components, color='r', linestyle='--', label='Elbow Point')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('images/without_missingness/BIC.png')\n",
    "plt.show()\n",
    "\n",
    "# Print optimal number of components\n",
    "print(\"Optimal number of components based on elbow point:\", optimal_n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Mixture Model with optimal number of components\n",
    "gmm = GaussianMixture(n_components=optimal_n_components, random_state=RANDOM_STATE)\n",
    "gmm.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(cgmm_subsets, 'cgmm', 10, gmm)\n",
    "\n",
    "cgmm_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_score = get_scoring(cgmm_subsets, 'cgmm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgmm_accuracy = get_accuracy(cgmm_subsets, 'cgmm', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['cgmm'] = {'score': cgmm_score, 'accuracy': np.round(cgmm_accuracy * 100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Perform Singular Value Decomposition (SVD) on training data\n",
    "svd = TruncatedSVD(n_components=min(X_train.shape), random_state=RANDOM_STATE)\n",
    "svd.fit(X_train)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Choose threshold to preserve 90% of total variance\n",
    "threshold_index = np.argmax(cumulative_variance_ratio >= 0.90)\n",
    "threshold = svd.singular_values_[threshold_index]\n",
    "\n",
    "print(f\"Starting threshold to preserve 90% of total variance: {threshold}\")\n",
    "\n",
    "# Analyze singular values\n",
    "singular_values = svd.singular_values_\n",
    "num_non_trivial = np.sum(singular_values > threshold)  # Choose a threshold to determine non-trivial singular values\n",
    "\n",
    "# Select latent space dimensionality\n",
    "latent_dim = num_non_trivial\n",
    "\n",
    "print(f\"Number of non-trivial singular values: {num_non_trivial}\")\n",
    "print(f\"Selected latent space dimensionality: {latent_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "inputs = Input(shape=(input_dim,))\n",
    "encoded = inputs\n",
    "encoded = Dense(128, activation='relu')(encoded)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "z_mean = Dense(latent_dim)(encoded)\n",
    "z_log_var = Dense(latent_dim)(encoded)\n",
    "\n",
    "# Reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Define the decoder\n",
    "decoded = z\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "outputs = Dense(input_dim)(decoded)\n",
    "\n",
    "# Create the VAE model\n",
    "vae = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "vae.compile(optimizer='adam', loss='mse')  # Use MSE as the reconstruction loss\n",
    "\n",
    "# Train the model\n",
    "history = vae.fit(X_train, X_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputing_missing_data(vae_subsets, 'vae', 10, vae)\n",
    "\n",
    "vae_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_score = get_scoring(vae_subsets, 'vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_accuracy = get_accuracy(vae_subsets, 'vae', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['vae'] = {'score': vae_score, 'accuracy': np.round(vae_accuracy * 100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Imputation Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_subsets = copy.deepcopy(list_of_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = load('generative_models\\gain_generator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40709</th>\n",
       "      <td>1.233014</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>1.296064</td>\n",
       "      <td>0.402504</td>\n",
       "      <td>0.137541</td>\n",
       "      <td>0.017879</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28468</th>\n",
       "      <td>0.061662</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>-1.261771</td>\n",
       "      <td>-0.083754</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14340</th>\n",
       "      <td>-0.509423</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>0.199849</td>\n",
       "      <td>-0.153219</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.088238</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20042</th>\n",
       "      <td>-2.039071</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>-0.652763</td>\n",
       "      <td>-1.125735</td>\n",
       "      <td>-0.187113</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55213</th>\n",
       "      <td>-1.589985</td>\n",
       "      <td>-0.733108</td>\n",
       "      <td>0.565254</td>\n",
       "      <td>-0.639477</td>\n",
       "      <td>-0.122182</td>\n",
       "      <td>-0.141297</td>\n",
       "      <td>-0.539322</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    gender    height    weight     ap_hi     ap_lo cholesterol  \\\n",
       "40709  1.233014 -0.733108  1.296064  0.402504  0.137541  0.017879   -0.539322   \n",
       "28468  0.061662 -0.733108 -1.261771 -0.083754  0.007679 -0.141297   -0.539322   \n",
       "14340 -0.509423 -0.733108  0.199849 -0.153219 -0.057251 -0.088238   -0.539322   \n",
       "20042 -2.039071 -0.733108 -0.652763 -1.125735 -0.187113 -0.141297   -0.539322   \n",
       "55213 -1.589985 -0.733108  0.565254 -0.639477 -0.122182 -0.141297   -0.539322   \n",
       "\n",
       "      gluc                                              smoke alco active  \n",
       "40709    0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0      0  \n",
       "28468    0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0      0  \n",
       "14340    0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0      1  \n",
       "20042    0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0      1  \n",
       "55213    0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    0      1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputing_missing_data(gain_subsets, 'gain', 10, gain)\n",
    "\n",
    "gain_subsets[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_score = get_scoring(gain_subsets, 'gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_accuracy = get_accuracy(gain_subsets, 'gain', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict['gain'] = {'score': gain_score, 'accuracy': np.round(gain_accuracy * 100, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "number_of_samples = 10\n",
    "\n",
    "# Define the directory where results are stored\n",
    "results_directory = \"results/without_missingness\"\n",
    "\n",
    "# Get the list of existing result files to determine the next run number\n",
    "existing_files = os.listdir(results_directory)\n",
    "run_numbers = [int(file.split(\"_\")[1].split(\".\")[0]) for file in existing_files if file.startswith(\"run_\")]\n",
    "\n",
    "# Determine the next run number\n",
    "next_run_number = max(run_numbers, default=0) + 1\n",
    "\n",
    "# Create a table for accuracy\n",
    "accuracy_table = [[\"\"] + list(results_dict.keys())]\n",
    "for i in range(len(next(iter(results_dict.values()))[\"accuracy\"])):\n",
    "    accuracy_table.append([i+1] + [results_dict[key][\"accuracy\"][i] for key in results_dict.keys()])\n",
    "\n",
    "# Generate the tabulated string for accuracy\n",
    "tabulated_accuracy_table = tabulate(accuracy_table, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "\n",
    "# Create a table for score\n",
    "score_table = [[\"\"] + list(results_dict.keys())]\n",
    "for i in range(len(next(iter(results_dict.values()))[\"score\"])):\n",
    "    score_table.append([i+1] + [results_dict[key][\"score\"].get(i, \"\") for key in results_dict.keys()])\n",
    "\n",
    "# Generate the tabulated string for score\n",
    "tabulated_score_table = tabulate(score_table, headers=\"firstrow\", tablefmt=\"grid\")\n",
    "\n",
    "print(tabulated_score_table)\n",
    "\n",
    "# Define the file name for the new result\n",
    "new_file_name = f\"run_{next_run_number}.txt\"\n",
    "file_path = os.path.join(results_directory, new_file_name)\n",
    "\n",
    "# Save both tables to the same file with separation\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(\"Fraction of data: \" + str(fraction_of_data * 100) + \"%\\n\" + \"Number of samples: \" + str(number_of_samples) + \"\\n\\n\" + tabulated_accuracy_table + \"\\n\\n\\n\" + tabulated_score_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
